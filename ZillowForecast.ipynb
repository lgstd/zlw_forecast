{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.78 ms\n"
     ]
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "time: 15.9 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc \n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 165 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 515 Âµs\n"
     ]
    }
   ],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded columns:\n",
      "['hashottuborspa', 'finishedsquarefeet6', 'basementsqft', 'taxdelinquencyflag', 'buildingclasstypeid', 'pooltypeid7', 'fireplaceflag', 'typeconstructiontypeid', 'pooltypeid10', 'taxdelinquencyyear', 'pooltypeid2', 'poolcnt', 'architecturalstyletypeid', 'poolsizesum', 'decktypeid', 'finishedsquarefeet13', 'yardbuildingsqft26', 'storytypeid']\n",
      "........\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Categorical features:\n",
      "['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'day_of_week', 'month_of_year', 'quarter', 'is_weekend', 'propertyzoningdesc_frq', 'propertyzoningdesc_len', 'regionidzip_ab', 'regionidzip_abc', 'regionidneighborhood_ab']\n",
      "........\n",
      "time: 1min 2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "drop_tax = False\n",
    "\n",
    "train2016 = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "if drop_tax:\n",
    "    # Avoids external bias\n",
    "    print('Removing tax features from 2017')\n",
    "    train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions2016 = pd.merge(train2016, properties2016, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions2017 = pd.merge(train2017, properties2017, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions = transactions2016.copy()#pd.concat([transactions2016, transactions2017], axis = 0)\n",
    "\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "transactions['taxdelinquencyflag'].replace('Y', 1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.0\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all\n",
    "del y_all\n",
    "del transactions\n",
    "del transactions2016\n",
    "del transactions2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MAE: -0.06863 (+/- 0.00137)\n",
      "time: 643 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_train_data, x_train_label)\n",
    "#y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "#y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "#predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "#predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "\n",
    "#print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_lr, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_lr.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_lr_valid\n",
    "#del y_pred_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0685395047302\n",
      "time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "model_lr.fit(x_train_data, x_train_label)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0679043276933\n",
      "time: 90.8 ms\n"
     ]
    }
   ],
   "source": [
    "model_lr.fit(x_train, y_train)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge MAE: -0.06848 (+/- 0.00140)\n",
      "time: 731 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "#model_br.fit(x_train, y_train)\n",
    "#y_pred_br_valid = model_br.predict(x_valid)\n",
    "#y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['BayesianRidge'] = model_br\n",
    "\n",
    "#predicted_mae_br_valid = mean_absolute_error(y_valid,       y_pred_br_valid)\n",
    "#predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "#print('BR MAE BayesianRidge Valid: %s \\nTrain: %s' % (predicted_mae_br_valid, predicted_mae_br_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_br, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_br.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "#del y_pred_br_valid\n",
    "#del y_pred_br_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0679043276933\n",
      "time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "model_br.fit(x_train_data, x_train_label)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0679043276933\n",
      "time: 158 ms\n"
     ]
    }
   ],
   "source": [
    "model_br.fit(x_train, y_train)\n",
    "print(mean_absolute_error(x_train_label, model_lr.predict(x_train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 39.5 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=1, random_state=2016, verbose=1, n_estimators=500, max_features=12)\n",
    "#model_rf.fit(x_train, y_train)\n",
    "#y_pred_rf_valid = model_rf.predict(x_valid)\n",
    "#y_pred_rf_train = model_rf.predict(x_train_data)\n",
    "models['RandomForest'] = model_rf\n",
    "\n",
    "#predicted_mae_rf_valid = mean_absolute_error(y_valid,       y_pred_rf_valid)\n",
    "#predicted_mae_rf_train = mean_absolute_error(x_train_label, y_pred_rf_train)\n",
    "\n",
    "#print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_rf, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_rf.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_rf_train\n",
    "#del y_pred_rf_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.07 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12)\n",
    "\n",
    "#model_et.fit(x_train, y_train)\n",
    "#y_pred_et_valid = model_et.predict(x_valid)\n",
    "#y_pred_et_train = model_et.predict(x_train_data)\n",
    "models['ExtraTrees'] = model_et\n",
    "\n",
    "#predicted_mae_et_valid = mean_absolute_error(y_valid,       y_pred_et_valid)\n",
    "#predicted_mae_et_train = mean_absolute_error(x_train_label, y_pred_et_train)\n",
    "\n",
    "#print('BR MAE ExtraTrees Valid: %s \\nTrain: %s' % (predicted_mae_et_valid, predicted_mae_et_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_et, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_et.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_et_valid\n",
    "#del y_pred_et_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 38.5 ms\n"
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.05 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_ab = AdaBoostRegressor()\n",
    "#model_ab.fit(x_train, y_train)\n",
    "#y_pred_ab_valid = model_ab.predict(x_valid)\n",
    "#y_pred_ab_train = model_ab.predict(x_train_data)\n",
    "models['AdaBoost'] = model_ab\n",
    "\n",
    "#predicted_mae_ab_valid = mean_absolute_error(y_valid,       y_pred_ab_valid)\n",
    "#predicted_mae_ab_train = mean_absolute_error(x_train_label, y_pred_ab_train)\n",
    "\n",
    "#print('BR MAE AdaBoost Valid: %s \\nTrain: %s' % (predicted_mae_ab_valid, predicted_mae_ab_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_ab, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_ab.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_ab_valid\n",
    "#del y_pred_ab_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index, loss='MAE'):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=630, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function=loss,\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cb, preds = cat_booster(x_train, y_train, x_train_data, x_train_label, cat_index)\n",
    "\n",
    "print('BR MAE CatBoost Valid: %s' % (mean_absolute_error(y_valid, preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostRegressor(\n",
    "            iterations=630, learning_rate=0.03,\n",
    "            depth=6, l2_leaf_reg=3,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE')\n",
    "\n",
    "models['CatBoost'] = model_cb\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_cb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.65 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor(\n",
    "             random_state=2016, verbose=1,\n",
    "             n_estimators=500, max_features=12, max_depth=8,\n",
    "             learning_rate=0.05, subsample=0.8)\n",
    "\n",
    "#model_gb.fit(x_train, y_train)\n",
    "#y_pred_gb_valid = model_gb.predict(x_valid)\n",
    "#y_pred_gb_train = model_gb.predict(x_train_data)\n",
    "models['GradientBoosting'] = model_gb\n",
    "\n",
    "#predicted_mae_gb_valid = mean_absolute_error(y_valid,       y_pred_gb_valid)\n",
    "#predicted_mae_gb_train = mean_absolute_error(x_train_label, y_pred_gb_train)\n",
    "\n",
    "#print('BR MAE GradientBoosting Valid: %s \\nTrain: %s' % (predicted_mae_gb_valid, predicted_mae_gb_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_gb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_gb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_gb_valid\n",
    "#del y_pred_gb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae',\n",
    "    'base_score':       y_mean,\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(x_train, label=y_train, missing=-1)\n",
    "#d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "#models['XGB'] = xgb_gs\n",
    "\n",
    "#del d_train\n",
    "#del d_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.93 ms\n"
     ]
    }
   ],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.08 s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,   # num_leaf\n",
    "    'min_data'         : 500,   # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,  # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "model_lgb = lgb.train(params_lg, d_train, 430)\n",
    "\n",
    "#model_lgb = light_gbm_folds(x_train, x_train_data, y_train, x_train_label, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neural network data...\n",
      "time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train_nn = imputer.transform(x_train.iloc[:, :])\n",
    "\n",
    "#imputer.fit(x_valid.iloc[:, :])\n",
    "#x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "#x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59 s\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "models['DNN'] = pipeline\n",
    "\n",
    "#print(mean_absolute_error(y_valid, pipeline.predict(x_valid_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "90275/90275 [==============================] - 11s - loss: 0.0683    \n",
      "Epoch 2/15\n",
      "90275/90275 [==============================] - 10s - loss: 0.0682    \n",
      "Epoch 3/15\n",
      "90275/90275 [==============================] - 13s - loss: 0.0682    \n",
      "Epoch 4/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 5/15\n",
      "90275/90275 [==============================] - 13s - loss: 0.0682    \n",
      "Epoch 6/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 7/15\n",
      "90275/90275 [==============================] - 15s - loss: 0.0682    \n",
      "Epoch 8/15\n",
      "90275/90275 [==============================] - 13s - loss: 0.0682    \n",
      "Epoch 9/15\n",
      "90275/90275 [==============================] - 13s - loss: 0.0682    \n",
      "Epoch 10/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 11/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 12/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 13/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 14/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n",
      "Epoch 15/15\n",
      "90275/90275 [==============================] - 14s - loss: 0.0682    \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ecd5b68997d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#yhat = lstm.predict(x_valid_lstm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test MAE: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 30s\n"
     ]
    }
   ],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = x_train_data.values.reshape((x_train_data.shape[0], 1, x_train_data.shape[1]))\n",
    "#x_valid_lstm = x_valid.values.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 50 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "#validation_data=(x_valid_lstm, y_valid)\n",
    "lstm.fit(x_train_lstm, x_train_label, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "#yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "#mae = mean_absolute_error(y_valid, yhat)\n",
    "#print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 518 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/ensemble.py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, stacker, base_models, include_features, cvgrid):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "        self.features = include_features\n",
    "        self.param_grid = cvgrid\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, include_features=False):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))       \n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold %d / %d ---', j + 1, self.n_folds)\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    \n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "\n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=self.param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        \n",
    "        if self.features:\n",
    "            S_train = np.append(X, S_train, 1)\n",
    "            \n",
    "        grid.fit(S_train, y)\n",
    "        \n",
    "        try:\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.stacker = grid.best_estimator_\n",
    "        \n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        if self.features:\n",
    "            S_test = np.append(X, np.zeros((X.shape[0], len(self.base_models))), 1)  \n",
    "            print('Using features of shape', S_test.shape)\n",
    "        else:\n",
    "            S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "            print('Using features of shape', S_test.shape)\n",
    "\n",
    "        for ind, c in enumerate(self.base_models):\n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            # Uses all features.\n",
    "            if self.features:\n",
    "                i = X.shape[1] + ind\n",
    "            else:\n",
    "                i = ind\n",
    "                \n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            print('--- Predicting For  #{}'.format(c))\n",
    "            \n",
    "            # Makes predictions for each model\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):    \n",
    "                if c not in ['XGB', 'LSTM']:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(X.reshape((X.shape[0], 1, X.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        y_pred = clf.predict(S_test)[:]\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test  = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('########## \\nFitting For Base Model {} \\n##########'.format(c))\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "                    \n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(T.reshape((T.shape[0], 1, T.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    data_pred = xg.DMatrix(T, missing=-1)\n",
    "                    S_test_i[:, j] = clf.predict(data_pred)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {'n_estimators':  [100],\n",
    "                      'learning_rate': [0.05],\n",
    "                      'subsample':     [0.75]}\n",
    "\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 550 Âµs\n"
     ]
    }
   ],
   "source": [
    "#del models['CatBoost']\n",
    "#del models['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting For Base Model LinearRegression ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "Elapsed: 0.01 minutes ---\n",
      "Fitting For Base Model BayesianRidge ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "Elapsed: 0.02 minutes ---\n",
      "Fitting For Base Model RandomForest ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 5.17 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 10.23 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 15.34 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 20.42 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 25.53 minutes ---\n",
      "Elapsed: 25.53 minutes ---\n",
      "Fitting For Base Model ExtraTrees ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    9.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 27.78 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 29.93 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 32.11 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 34.29 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 36.53 minutes ---\n",
      "Elapsed: 36.53 minutes ---\n",
      "Fitting For Base Model AdaBoost ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 36.61 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 36.7 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 36.79 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 36.92 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 37.02 minutes ---\n",
      "Elapsed: 37.02 minutes ---\n",
      "Fitting For Base Model GradientBoosting ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0069           0.0000            2.35m\n",
      "         2           0.0069           0.0000            2.36m\n",
      "         3           0.0068           0.0000            2.45m\n",
      "         4           0.0068           0.0000            2.34m\n",
      "         5           0.0068           0.0000            2.33m\n",
      "         6           0.0068           0.0000            2.33m\n",
      "         7           0.0068           0.0000            2.27m\n",
      "         8           0.0068           0.0000            2.23m\n",
      "         9           0.0067           0.0000            2.20m\n",
      "        10           0.0067           0.0000            2.20m\n",
      "        20           0.0065           0.0000            2.18m\n",
      "        30           0.0064           0.0000            2.09m\n",
      "        40           0.0064           0.0000            2.01m\n",
      "        50           0.0062           0.0000            1.97m\n",
      "        60           0.0061          -0.0000            1.90m\n",
      "        70           0.0061          -0.0000            1.86m\n",
      "        80           0.0061          -0.0000            1.84m\n",
      "        90           0.0059          -0.0000            1.79m\n",
      "       100           0.0059          -0.0000            1.73m\n",
      "       200           0.0053          -0.0000            1.25m\n",
      "       300           0.0049          -0.0000           49.73s\n",
      "       400           0.0045          -0.0000           24.86s\n",
      "       500           0.0041          -0.0000            0.00s\n",
      "Elapsed: 39.13 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0070           0.0000            2.51m\n",
      "         2           0.0070           0.0000            2.47m\n",
      "         3           0.0070           0.0000            2.47m\n",
      "         4           0.0069           0.0000            2.45m\n",
      "         5           0.0069           0.0000            2.40m\n",
      "         6           0.0068           0.0000            2.36m\n",
      "         7           0.0068           0.0000            2.36m\n",
      "         8           0.0068           0.0000            2.35m\n",
      "         9           0.0068           0.0000            2.32m\n",
      "        10           0.0068           0.0000            2.29m\n",
      "        20           0.0066           0.0000            2.27m\n",
      "        30           0.0065           0.0000            2.20m\n",
      "        40           0.0063          -0.0000            2.14m\n",
      "        50           0.0063           0.0000            2.07m\n",
      "        60           0.0061          -0.0000            2.01m\n",
      "        70           0.0061          -0.0000            1.92m\n",
      "        80           0.0060          -0.0000            1.86m\n",
      "        90           0.0059          -0.0000            1.82m\n",
      "       100           0.0059           0.0000            1.76m\n",
      "       200           0.0053          -0.0000            1.29m\n",
      "       300           0.0049          -0.0000           51.65s\n",
      "       400           0.0044          -0.0000           26.45s\n",
      "       500           0.0040          -0.0000            0.00s\n",
      "Elapsed: 41.38 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0069           0.0000            2.78m\n",
      "         2           0.0069           0.0000            2.72m\n",
      "         3           0.0069           0.0000            2.74m\n",
      "         4           0.0068           0.0000            2.60m\n",
      "         5           0.0069           0.0000            2.43m\n",
      "         6           0.0068           0.0000            2.40m\n",
      "         7           0.0068           0.0000            2.37m\n",
      "         8           0.0068           0.0000            2.34m\n",
      "         9           0.0068           0.0000            2.36m\n",
      "        10           0.0067           0.0000            2.34m\n",
      "        20           0.0065           0.0000            2.41m\n",
      "        30           0.0064           0.0000            2.39m\n",
      "        40           0.0063           0.0000            2.33m\n",
      "        50           0.0062           0.0000            2.23m\n",
      "        60           0.0061          -0.0000            2.14m\n",
      "        70           0.0061          -0.0000            2.05m\n",
      "        80           0.0060          -0.0000            1.97m\n",
      "        90           0.0059           0.0000            1.89m\n",
      "       100           0.0059           0.0000            1.81m\n",
      "       200           0.0053          -0.0000            1.37m\n",
      "       300           0.0049          -0.0000           53.21s\n",
      "       400           0.0044          -0.0000           26.55s\n",
      "       500           0.0041          -0.0000            0.00s\n",
      "Elapsed: 43.61 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0069           0.0000            2.51m\n",
      "         2           0.0070           0.0000            2.27m\n",
      "         3           0.0070           0.0000            2.45m\n",
      "         4           0.0069           0.0000            2.57m\n",
      "         5           0.0069           0.0000            2.61m\n",
      "         6           0.0068           0.0000            2.67m\n",
      "         7           0.0069           0.0000            2.67m\n",
      "         8           0.0068           0.0000            2.64m\n",
      "         9           0.0068           0.0000            2.65m\n",
      "        10           0.0067           0.0000            2.59m\n",
      "        20           0.0066           0.0000            2.30m\n",
      "        30           0.0064           0.0000            2.21m\n",
      "        40           0.0064          -0.0000            2.10m\n",
      "        50           0.0063          -0.0000            2.00m\n",
      "        60           0.0062          -0.0000            1.90m\n",
      "        70           0.0061          -0.0000            1.83m\n",
      "        80           0.0061           0.0000            1.79m\n",
      "        90           0.0059           0.0000            1.73m\n",
      "       100           0.0059          -0.0000            1.67m\n",
      "       200           0.0053          -0.0000            1.24m\n",
      "       300           0.0049          -0.0000           49.37s\n",
      "       400           0.0044          -0.0000           25.06s\n",
      "       500           0.0041          -0.0000            0.00s\n",
      "Elapsed: 45.75 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0069           0.0000            2.56m\n",
      "         2           0.0069           0.0000            2.54m\n",
      "         3           0.0069           0.0000            2.68m\n",
      "         4           0.0069           0.0000            2.68m\n",
      "         5           0.0069           0.0000            2.72m\n",
      "         6           0.0068           0.0000            2.68m\n",
      "         7           0.0068           0.0000            2.57m\n",
      "         8           0.0068           0.0000            2.53m\n",
      "         9           0.0068           0.0000            2.53m\n",
      "        10           0.0067           0.0000            2.53m\n",
      "        20           0.0066           0.0000            2.42m\n",
      "        30           0.0065           0.0000            2.20m\n",
      "        40           0.0063          -0.0000            2.16m\n",
      "        50           0.0062           0.0000            2.10m\n",
      "        60           0.0062           0.0000            2.03m\n",
      "        70           0.0061          -0.0000            1.95m\n",
      "        80           0.0060          -0.0000            1.90m\n",
      "        90           0.0060           0.0000            1.85m\n",
      "       100           0.0059          -0.0000            1.79m\n",
      "       200           0.0054          -0.0000            1.27m\n",
      "       300           0.0049          -0.0000           51.56s\n",
      "       400           0.0044          -0.0000           25.47s\n",
      "       500           0.0040          -0.0000            0.00s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 47.89 minutes ---\n",
      "Elapsed: 47.89 minutes ---\n",
      "Fitting For Base Model LightGBM ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 47.92 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 47.95 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 47.98 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 48.02 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 48.05 minutes ---\n",
      "Elapsed: 48.05 minutes ---\n",
      "Fitting For Base Model DNN ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':  [50, 100, 150],\n",
    "              'learning_rate': [0.03, 0.05, 0.07],\n",
    "              'subsample':     [0.5, 0.75, 1]\n",
    "             }\n",
    "\n",
    "ensemble = Ensemble(n_folds=5,\n",
    "                    stacker=GradientBoostingRegressor(random_state=2016, verbose=1),\n",
    "                    base_models=models, include_features=False, cvgrid=param_grid)\n",
    "                    \n",
    "#model_ensemble = ensemble.fit_predict(x_train[:100], y_train[:100], x_valid)\n",
    "# MAE 0.0653212760898 - lr 0.03, nest = 50, subsample: 0.5\n",
    "ensemble.fit(x_train, y_train)\n",
    "#final_prediction = ensemble.predict(x_valid)\n",
    "#print('MAE', mean_absolute_error(y_valid, final_prediction))\n",
    "\n",
    "#del final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, K, randomise = False):\n",
    "    \"\"\"Generates K (training, validation) pairs from the items in X.\"\"\"\n",
    "    \n",
    "    if randomise: from random import shuffle; X=list(X); shuffle(X)\n",
    "    for k in range(K):\n",
    "        training   = [x for i, x in enumerate(X) if i % K != k]\n",
    "        validation = [x for i, x in enumerate(X) if i % K == k]\n",
    "        \n",
    "        yield training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for training, validation in k_fold_cross_validation(x_train_data.values, K=5):\n",
    "    pred_k = ensemble.predict(validation)\n",
    "    print('MAE',mean_absolute_error(x_train_label[index:index+len(validation)], pred_k))\n",
    "    index += len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = transactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "all_preds = pd.DataFrame()\n",
    "for m in test_dates.keys():\n",
    "    # Building predictions.\n",
    "    print('Processing', m)\n",
    "    x_predict = transactions.copy()\n",
    "    x_predict = complex_features(x_predict)\n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    \n",
    "    print('Cleaning data')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict = x_predict.fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict[m] = xgb_gs2.predict(xg.DMatrix(x_predict[best_columns]))\n",
    "    all_preds[m] = x_predict[m].copy()\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict[list(test_dates.keys())] = all_preds[list(test_dates.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = '201610'\n",
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample.to_csv('submission4.csv',index=False)\n",
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {'201610': pd.Timestamp('2016-09-30'),\n",
    "              '201611': pd.Timestamp('2016-10-31'),\n",
    "              '201612': pd.Timestamp('2016-11-30')}\n",
    "\n",
    "x_predict = transactions.copy()\n",
    "x_predict = complex_features(x_predict)\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    print('Processing', m)  \n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    x_predict = x_predict[best_columns].fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    # 5 iterations cat booster for each prediction.\n",
    "    x_predict[m] = cat_booster(x_train, y_train, x_predict.values)\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "del x_predict\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "#new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

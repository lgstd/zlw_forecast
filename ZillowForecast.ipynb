{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded columns:\n",
      "['taxdelinquencyyear', 'buildingclasstypeid', 'yardbuildingsqft26', 'poolcnt', 'pooltypeid2', 'finishedsquarefeet13', 'poolsizesum', 'storytypeid', 'pooltypeid10', 'architecturalstyletypeid', 'decktypeid', 'typeconstructiontypeid', 'pooltypeid7', 'finishedsquarefeet6', 'taxdelinquencyflag', 'hashottuborspa', 'basementsqft', 'fireplaceflag']\n",
      "........\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Categorical features:\n",
      "['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'day_of_week', 'month_of_year', 'quarter', 'is_weekend', 'propertyzoningdesc_frq', 'propertyzoningdesc_len', 'regionidzip_ab', 'regionidzip_abc', 'regionidneighborhood_ab']\n",
      "........\n",
      "time: 31.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "\n",
    "train = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../Data/properties_2016.csv')\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions = pd.merge(train, prop, how='left', on=['parcelid']).sample(frac=1)\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "transactions['taxdelinquencyflag'].replace('Y',1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.1\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all\n",
    "del y_all\n",
    "del transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE LR Valid: 0.0655499689458 Train: 0.0681734572472\n",
      "time: 171 ms\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_train, y_train)\n",
    "y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "\n",
    "print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "\n",
    "del y_pred_lr_valid\n",
    "del y_pred_lr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR MAE BayesianRidge Valid: 0.065548413949 \n",
      "Train: 0.0681805346714\n",
      "time: 215 ms\n"
     ]
    }
   ],
   "source": [
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "model_br.fit(x_train, y_train)\n",
    "y_pred_br_valid = model_br.predict(x_valid)\n",
    "y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['BayesianRidge'] = model_br\n",
    "\n",
    "predicted_mae_br_valid = mean_absolute_error(y_valid,       y_pred_br_valid)\n",
    "predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "print('BR MAE BayesianRidge Valid: %s \\nTrain: %s' % (predicted_mae_br_valid, predicted_mae_br_train))\n",
    "\n",
    "del y_pred_br_valid\n",
    "del y_pred_br_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  5.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR MAE RandomForest Valid: 0.0656375533565 \n",
      "Train: 0.035217631714\n",
      "time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=1, random_state=2016, verbose=1, n_estimators=500, \n",
    "                                 max_features=12)\n",
    "model_rf.fit(x_train, y_train)\n",
    "y_pred_rf_valid = model_rf.predict(x_valid)\n",
    "y_pred_rf_train = model_rf.predict(x_train_data)\n",
    "models['RandomForest'] = model_rf\n",
    "\n",
    "predicted_mae_rf_valid = mean_absolute_error(y_valid,       y_pred_rf_valid)\n",
    "predicted_mae_rf_train = mean_absolute_error(x_train_label, y_pred_rf_train)\n",
    "\n",
    "print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\n",
    "\n",
    "del y_pred_rf_train\n",
    "del y_pred_rf_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   21.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR MAE ExtraTrees Valid: 0.0665878837395 \n",
      "Train: 0.0159463449264\n",
      "time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12)\n",
    "\n",
    "model_et.fit(x_train, y_train)\n",
    "y_pred_et_valid = model_et.predict(x_valid)\n",
    "y_pred_et_train = model_et.predict(x_train_data)\n",
    "models['ExtraTrees'] = model_et\n",
    "\n",
    "predicted_mae_et_valid = mean_absolute_error(y_valid,       y_pred_et_valid)\n",
    "predicted_mae_et_train = mean_absolute_error(x_train_label, y_pred_et_train)\n",
    "\n",
    "print('BR MAE ExtraTrees Valid: %s \\nTrain: %s' % (predicted_mae_et_valid, predicted_mae_et_train))\n",
    "\n",
    "del y_pred_et_valid\n",
    "del y_pred_et_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "model_svr_lin = SVR(kernel='linear', C=1e3)\n",
    "model_svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "\n",
    "y_rbf  =  model_svr_rbf.fit(x_train, y_train).predict(x_valid)\n",
    "y_lin  =  model_svr_lin.fit(x_train, y_train).predict(x_valid)\n",
    "y_poly = model_svr_poly.fit(x_train, y_train).predict(x_valid)\n",
    "\n",
    "scores_r = cross_validation.cross_val_score(model_svr_rbf,  x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "scores_l = cross_validation.cross_val_score(model_svr_lin,  x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "scores_p = cross_validation.cross_val_score(model_svr_poly, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_rbf.__class__.__name__,  scores_r.mean(), scores_r.std() * 2))\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_lin.__class__.__name__,  scores_l.mean(), scores_l.std() * 2))\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_svr_poly.__class__.__name__, scores_p.mean(), scores_p.std() * 2))\n",
    "\n",
    "print('RBF',   mean_absolute_error(y_valid, y_rbf))\n",
    "print('Linear',mean_absolute_error(y_valid, y_lin))\n",
    "print('Poly',  mean_absolute_error(y_valid, y_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR MAE AdaBoost Valid: 0.0668425181751 \n",
      "Train: 0.0695206978103\n",
      "time: 8.38 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_ab = AdaBoostRegressor()\n",
    "model_ab.fit(x_train, y_train)\n",
    "y_pred_ab_valid = model_ab.predict(x_valid)\n",
    "y_pred_ab_train = model_ab.predict(x_train_data)\n",
    "models['AdaBoost'] = model_ab\n",
    "\n",
    "predicted_mae_ab_valid = mean_absolute_error(y_valid,       y_pred_ab_valid)\n",
    "predicted_mae_ab_train = mean_absolute_error(x_train_label, y_pred_ab_train)\n",
    "\n",
    "print('BR MAE AdaBoost Valid: %s \\nTrain: %s' % (predicted_mae_ab_valid, predicted_mae_ab_train))\n",
    "\n",
    "del y_pred_ab_valid\n",
    "del y_pred_ab_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 13.8 ms\n"
     ]
    }
   ],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index, loss='MAE'):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=630, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function=loss,\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CAT Boost Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:33<00:00, 30.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.0522779106976\n",
      "Valid MAE: 0.0650190201683\n",
      "BR MAE CatBoost Valid: 0.0650190201683\n",
      "time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "model_cb, preds = cat_booster(x_train, y_train, x_valid, y_valid, cat_index)\n",
    "\n",
    "models['CatBoost'] = model_cb\n",
    "\n",
    "print('BR MAE CatBoost Valid: %s' % (mean_absolute_error(y_valid, preds)))\n",
    "\n",
    "del preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoostRegressor MAE: -0.06777 (+/- 0.00186)\n",
      "time: 28.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   28.6s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validation.cross_val_score(model_cb, x_train_data, x_train_label, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0070           0.0000            2.92m\n",
      "         2           0.0069           0.0000            2.80m\n",
      "         3           0.0069           0.0000            2.75m\n",
      "         4           0.0069           0.0000            2.79m\n",
      "         5           0.0068           0.0000            2.80m\n",
      "         6           0.0069           0.0000            2.81m\n",
      "         7           0.0068           0.0000            2.69m\n",
      "         8           0.0068           0.0000            2.70m\n",
      "         9           0.0068           0.0000            2.69m\n",
      "        10           0.0068           0.0000            2.68m\n",
      "        20           0.0067           0.0000            2.55m\n",
      "        30           0.0065          -0.0000            2.49m\n",
      "        40           0.0064           0.0000            2.46m\n",
      "        50           0.0063          -0.0000            2.38m\n",
      "        60           0.0063          -0.0000            2.30m\n",
      "        70           0.0061           0.0000            2.21m\n",
      "        80           0.0061          -0.0000            2.14m\n",
      "        90           0.0060           0.0000            2.05m\n",
      "       100           0.0059          -0.0000            1.97m\n",
      "       200           0.0054          -0.0000            1.40m\n",
      "       300           0.0050          -0.0000           56.01s\n",
      "       400           0.0046          -0.0000           27.95s\n",
      "       500           0.0042          -0.0000            0.00s\n",
      "BR MAE GradientBoosting Valid: 0.0656477715072 \n",
      "Train: 0.0587680676837\n",
      "time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor(\n",
    "             random_state=2016, verbose=1,\n",
    "             n_estimators=500, max_features=12, max_depth=8,\n",
    "             learning_rate=0.05, subsample=0.8)\n",
    "\n",
    "model_gb.fit(x_train, y_train)\n",
    "y_pred_gb_valid = model_gb.predict(x_valid)\n",
    "y_pred_gb_train = model_gb.predict(x_train_data)\n",
    "models['GradientBoosting'] = model_gb\n",
    "\n",
    "predicted_mae_gb_valid = mean_absolute_error(y_valid,       y_pred_gb_valid)\n",
    "predicted_mae_gb_train = mean_absolute_error(x_train_label, y_pred_gb_train)\n",
    "\n",
    "print('BR MAE GradientBoosting Valid: %s \\nTrain: %s' % (predicted_mae_gb_valid, predicted_mae_gb_train))\n",
    "\n",
    "del y_pred_gb_valid\n",
    "del y_pred_gb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053332\tvalid-mae:0.065677\n",
      "Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mae:0.052084\tvalid-mae:0.06506\n",
      "[100]\ttrain-mae:0.051548\tvalid-mae:0.065021\n",
      "[150]\ttrain-mae:0.051058\tvalid-mae:0.065019\n",
      "Stopping. Best iteration:\n",
      "[94]\ttrain-mae:0.051603\tvalid-mae:0.065\n",
      "\n",
      "time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae',\n",
    "    'base_score':       y_mean,\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(x_train, label=y_train, missing=-1)\n",
    "d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "xgb_gs = xg.train(params_xgb, d_train, len(x_valid), watchlist, early_stopping_rounds=100, verbose_eval=50)\n",
    "models['XGB'] = xgb_gs\n",
    "\n",
    "del d_train\n",
    "del d_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.6 ms\n"
     ]
    }
   ],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Light GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.0533712410193\n",
      "Valid MAE: 0.0658219877093\n",
      "time: 38.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,       # num_leaf\n",
    "    'min_data'         : 500,         # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,     # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "\n",
    "model_lgb = light_gbm_folds(x_train, x_valid, y_train, y_valid, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neural network data...\n",
      "time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train_nn = imputer.transform(x_train.iloc[:, :])\n",
    "\n",
    "imputer.fit(x_valid.iloc[:, :])\n",
    "x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "models['DNN'] = pipeline\n",
    "\n",
    "print(mean_absolute_error(y_valid, pipeline.predict(x_valid_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81247 samples, validate on 9028 samples\n",
      "Epoch 1/15\n",
      "81247/81247 [==============================] - 16s - loss: 0.0686 - val_loss: 0.0658\n",
      "Epoch 2/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0685 - val_loss: 0.0657\n",
      "Epoch 3/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 4/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 5/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 6/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 7/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 8/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 9/15\n",
      "81247/81247 [==============================] - 16s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 10/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0657\n",
      "Epoch 11/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 12/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 13/15\n",
      "81247/81247 [==============================] - 14s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 14/15\n",
      "81247/81247 [==============================] - 15s - loss: 0.0684 - val_loss: 0.0658\n",
      "Epoch 15/15\n",
      "81247/81247 [==============================] - 15s - loss: 0.0684 - val_loss: 0.0658\n",
      "Test MAE: 0.066\n",
      "time: 3min 47s\n"
     ]
    }
   ],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = x_train_data.values.reshape((x_train_data.shape[0], 1, x_train_data.shape[1]))\n",
    "x_valid_lstm = x_valid.values.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 50 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "lstm.fit(x_train_lstm, x_train_label, epochs=15, batch_size=50, validation_data=(x_valid_lstm, y_valid), verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "mae = mean_absolute_error(y_valid, yhat)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 362 ms\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/ensemble.py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))       \n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold %d / %d ---', j+1, self.n_folds)\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "\n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        clf = self.stacker\n",
    "        clf.fit(S_train, y)\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(X.reshape((X.shape[0], 1, X.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict(xg.DMatrix(X, missing=-1))[:]\n",
    "                \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        y_pred = clf.predict(S_test)[:]\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test  = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "                    \n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(T.reshape((T.shape[0], 1, T.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(xg.DMatrix(T, missing=-1))[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {'n_estimators':  [100],\n",
    "                      'learning_rate': [0.05],\n",
    "                      'subsample':     [0.75]}\n",
    "        \n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting For Base Model LinearRegression ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "Fitting For Base Model BayesianRidge ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "Elapsed: 0.0 minutes ---\n",
      "Fitting For Base Model RandomForest ---\n",
      "--- Fitting For Fold #1 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.05 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.1 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.15 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.19 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.24 minutes ---\n",
      "Elapsed: 0.24 minutes ---\n",
      "Fitting For Base Model ExtraTrees ---\n",
      "--- Fitting For Fold #1 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.27 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.3 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.33 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.36 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.4 minutes ---\n",
      "Elapsed: 0.4 minutes ---\n",
      "Fitting For Base Model GradientBoosting ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0074          -0.0001            0.93s\n",
      "         2           0.0073          -0.0001            0.85s\n",
      "         3           0.0071          -0.0000            0.82s\n",
      "         4           0.0072          -0.0000            0.84s\n",
      "         5           0.0070          -0.0000            0.81s\n",
      "         6           0.0067          -0.0000            0.80s\n",
      "         7           0.0063          -0.0000            0.80s\n",
      "         8           0.0066          -0.0000            0.83s\n",
      "         9           0.0066           0.0000            0.82s\n",
      "        10           0.0061          -0.0000            0.83s\n",
      "        20           0.0043          -0.0000            0.76s\n",
      "        30           0.0035          -0.0000            0.76s\n",
      "        40           0.0028          -0.0000            0.76s\n",
      "        50           0.0022          -0.0000            0.75s\n",
      "        60           0.0018           0.0000            0.73s\n",
      "        70           0.0016          -0.0000            0.72s\n",
      "        80           0.0012          -0.0000            0.71s\n",
      "        90           0.0010          -0.0000            0.69s\n",
      "       100           0.0008          -0.0000            0.67s\n",
      "       200           0.0001          -0.0000            0.53s\n",
      "       300           0.0000          -0.0000            0.37s\n",
      "       400           0.0000          -0.0000            0.18s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "Elapsed: 0.41 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0072          -0.0000            0.68s\n",
      "         2           0.0075          -0.0000            0.68s\n",
      "         3           0.0074          -0.0000            0.76s\n",
      "         4           0.0066          -0.0000            0.72s\n",
      "         5           0.0067           0.0000            0.69s\n",
      "         6           0.0067          -0.0001            0.69s\n",
      "         7           0.0066          -0.0001            0.70s\n",
      "         8           0.0064          -0.0000            0.71s\n",
      "         9           0.0063          -0.0001            0.71s\n",
      "        10           0.0057          -0.0000            0.72s\n",
      "        20           0.0049          -0.0000            0.74s\n",
      "        30           0.0038           0.0000            0.74s\n",
      "        40           0.0030          -0.0000            0.72s\n",
      "        50           0.0022          -0.0000            0.72s\n",
      "        60           0.0017          -0.0000            0.70s\n",
      "        70           0.0014          -0.0000            0.69s\n",
      "        80           0.0012          -0.0000            0.67s\n",
      "        90           0.0009           0.0000            0.65s\n",
      "       100           0.0008          -0.0000            0.64s\n",
      "       200           0.0001          -0.0000            0.52s\n",
      "       300           0.0000          -0.0000            0.35s\n",
      "       400           0.0000          -0.0000            0.18s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "Elapsed: 0.43 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0071          -0.0001            1.46s\n",
      "         2           0.0073          -0.0000            1.40s\n",
      "         3           0.0068          -0.0000            1.31s\n",
      "         4           0.0066          -0.0000            1.20s\n",
      "         5           0.0068          -0.0000            1.17s\n",
      "         6           0.0062          -0.0000            1.12s\n",
      "         7           0.0064           0.0000            1.12s\n",
      "         8           0.0058          -0.0001            1.12s\n",
      "         9           0.0059          -0.0000            1.09s\n",
      "        10           0.0055          -0.0001            1.11s\n",
      "        20           0.0042          -0.0000            0.96s\n",
      "        30           0.0033          -0.0000            0.93s\n",
      "        40           0.0025          -0.0000            0.93s\n",
      "        50           0.0018           0.0000            0.92s\n",
      "        60           0.0014          -0.0000            0.87s\n",
      "        70           0.0012           0.0000            0.85s\n",
      "        80           0.0009          -0.0000            0.81s\n",
      "        90           0.0007          -0.0000            0.79s\n",
      "       100           0.0006           0.0000            0.77s\n",
      "       200           0.0001          -0.0000            0.57s\n",
      "       300           0.0000          -0.0000            0.38s\n",
      "       400           0.0000          -0.0000            0.19s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "Elapsed: 0.45 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0070          -0.0001            1.23s\n",
      "         2           0.0074          -0.0001            1.10s\n",
      "         3           0.0067          -0.0000            0.98s\n",
      "         4           0.0062          -0.0001            0.94s\n",
      "         5           0.0065          -0.0000            0.92s\n",
      "         6           0.0064           0.0000            0.90s\n",
      "         7           0.0060           0.0000            0.92s\n",
      "         8           0.0057          -0.0000            0.92s\n",
      "         9           0.0055          -0.0000            0.90s\n",
      "        10           0.0060          -0.0001            0.91s\n",
      "        20           0.0043           0.0000            0.86s\n",
      "        30           0.0033          -0.0000            0.83s\n",
      "        40           0.0026          -0.0000            0.80s\n",
      "        50           0.0019          -0.0000            0.80s\n",
      "        60           0.0016          -0.0000            0.78s\n",
      "        70           0.0013          -0.0000            0.76s\n",
      "        80           0.0010          -0.0000            0.74s\n",
      "        90           0.0008          -0.0000            0.72s\n",
      "       100           0.0007          -0.0000            0.71s\n",
      "       200           0.0001          -0.0000            0.54s\n",
      "       300           0.0000          -0.0000            0.36s\n",
      "       400           0.0000          -0.0000            0.18s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "Elapsed: 0.47 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0077           0.0000            0.84s\n",
      "         2           0.0070           0.0000            0.80s\n",
      "         3           0.0068           0.0000            0.79s\n",
      "         4           0.0068           0.0000            0.80s\n",
      "         5           0.0059          -0.0000            0.82s\n",
      "         6           0.0060           0.0001            0.85s\n",
      "         7           0.0058          -0.0000            0.83s\n",
      "         8           0.0057          -0.0000            0.84s\n",
      "         9           0.0050          -0.0000            0.86s\n",
      "        10           0.0054          -0.0000            0.86s\n",
      "        20           0.0044          -0.0000            0.79s\n",
      "        30           0.0032          -0.0000            0.78s\n",
      "        40           0.0027           0.0000            0.75s\n",
      "        50           0.0022          -0.0000            0.72s\n",
      "        60           0.0017          -0.0000            0.71s\n",
      "        70           0.0014          -0.0000            0.71s\n",
      "        80           0.0011           0.0000            0.71s\n",
      "        90           0.0010          -0.0000            0.69s\n",
      "       100           0.0007          -0.0000            0.69s\n",
      "       200           0.0001          -0.0000            0.53s\n",
      "       300           0.0000          -0.0000            0.36s\n",
      "       400           0.0000          -0.0000            0.18s\n",
      "       500           0.0000          -0.0000            0.00s\n",
      "Elapsed: 0.49 minutes ---\n",
      "Elapsed: 0.49 minutes ---\n",
      "Fitting For Base Model AdaBoost ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Elapsed: 0.49 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 0.49 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 0.49 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Fitting For Base Model XGB ---\n",
      "--- Fitting For Fold #1 / 5 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Fitting For Base Model LightGBM ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Elapsed: 0.5 minutes ---\n",
      "Fitting For Base Model DNN ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Elapsed: 0.82 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Elapsed: 1.14 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Elapsed: 1.46 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Elapsed: 1.78 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Elapsed: 2.11 minutes ---\n",
      "Elapsed: 2.11 minutes ---\n",
      "Fitting For Base Model LSTM ---\n",
      "--- Fitting For Fold #1 / 5 ---\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 0s - loss: 0.0556     \n",
      "Elapsed: 2.17 minutes ---\n",
      "--- Fitting For Fold #2 / 5 ---\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 0s - loss: 0.0539     \n",
      "Elapsed: 2.23 minutes ---\n",
      "--- Fitting For Fold #3 / 5 ---\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 0s - loss: 0.0538     \n",
      "Elapsed: 2.29 minutes ---\n",
      "--- Fitting For Fold #4 / 5 ---\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 0s - loss: 0.0541     \n",
      "Elapsed: 2.35 minutes ---\n",
      "--- Fitting For Fold #5 / 5 ---\n",
      "Epoch 1/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 2/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 3/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 4/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 5/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 6/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 7/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 8/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 9/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 10/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 11/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 12/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 13/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 14/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Epoch 15/15\n",
      "800/800 [==============================] - 0s - loss: 0.0537     \n",
      "Elapsed: 2.42 minutes ---\n",
      "Elapsed: 2.42 minutes ---\n",
      "--- Base Models Trained: 2.42 minutes ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] learning_rate=0.05, n_estimators=100, subsample=0.75 ............\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0074           0.0000            0.09s\n",
      "         2           0.0056          -0.0000            0.09s\n",
      "         3           0.0068           0.0001            0.08s\n",
      "         4           0.0074           0.0000            0.08s\n",
      "         5           0.0073           0.0000            0.08s\n",
      "         6           0.0065          -0.0000            0.08s\n",
      "         7           0.0069          -0.0000            0.08s\n",
      "         8           0.0072           0.0000            0.08s\n",
      "         9           0.0067          -0.0000            0.08s\n",
      "        10           0.0069          -0.0000            0.08s\n",
      "        20           0.0061          -0.0000            0.06s\n",
      "        30           0.0056          -0.0000            0.05s\n",
      "        40           0.0061          -0.0000            0.04s\n",
      "        50           0.0057          -0.0000            0.04s\n",
      "        60           0.0054          -0.0000            0.03s\n",
      "        70           0.0048          -0.0000            0.02s\n",
      "        80           0.0052          -0.0000            0.01s\n",
      "        90           0.0043          -0.0000            0.01s\n",
      "       100           0.0043           0.0000            0.00s\n",
      "[CV]  learning_rate=0.05, n_estimators=100, subsample=0.75, score=-0.064707 -   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100, subsample=0.75 ............\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0083          -0.0000            0.10s\n",
      "         2           0.0068          -0.0000            0.09s\n",
      "         3           0.0078           0.0000            0.09s\n",
      "         4           0.0078           0.0000            0.08s\n",
      "         5           0.0077           0.0000            0.08s\n",
      "         6           0.0072          -0.0000            0.08s\n",
      "         7           0.0071           0.0000            0.08s\n",
      "         8           0.0075          -0.0000            0.08s\n",
      "         9           0.0074           0.0000            0.08s\n",
      "        10           0.0073          -0.0000            0.08s\n",
      "        20           0.0074          -0.0000            0.06s\n",
      "        30           0.0061           0.0000            0.05s\n",
      "        40           0.0069          -0.0000            0.04s\n",
      "        50           0.0063          -0.0000            0.04s\n",
      "        60           0.0059          -0.0000            0.03s\n",
      "        70           0.0054          -0.0000            0.02s\n",
      "        80           0.0058          -0.0000            0.01s\n",
      "        90           0.0050          -0.0000            0.01s\n",
      "       100           0.0051          -0.0000            0.00s\n",
      "[CV]  learning_rate=0.05, n_estimators=100, subsample=0.75, score=-0.055284 -   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100, subsample=0.75 ............\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0084          -0.0000            0.08s\n",
      "         2           0.0072           0.0000            0.08s\n",
      "         3           0.0076          -0.0000            0.08s\n",
      "         4           0.0074           0.0000            0.08s\n",
      "         5           0.0077          -0.0000            0.08s\n",
      "         6           0.0070          -0.0000            0.08s\n",
      "         7           0.0069           0.0000            0.08s\n",
      "         8           0.0078          -0.0000            0.08s\n",
      "         9           0.0074          -0.0001            0.08s\n",
      "        10           0.0073          -0.0000            0.08s\n",
      "        20           0.0072          -0.0000            0.06s\n",
      "        30           0.0060          -0.0000            0.05s\n",
      "        40           0.0067          -0.0000            0.05s\n",
      "        50           0.0065          -0.0000            0.04s\n",
      "        60           0.0058          -0.0000            0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        70           0.0053          -0.0000            0.02s\n",
      "        80           0.0054          -0.0000            0.02s\n",
      "        90           0.0049          -0.0000            0.01s\n",
      "       100           0.0048          -0.0000            0.00s\n",
      "[CV]  learning_rate=0.05, n_estimators=100, subsample=0.75, score=-0.054956 -   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100, subsample=0.75 ............\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0082          -0.0000            0.08s\n",
      "         2           0.0077          -0.0000            0.08s\n",
      "         3           0.0077          -0.0000            0.08s\n",
      "         4           0.0077           0.0000            0.08s\n",
      "         5           0.0082          -0.0000            0.10s\n",
      "         6           0.0075           0.0000            0.10s\n",
      "         7           0.0071           0.0000            0.10s\n",
      "         8           0.0075          -0.0000            0.10s\n",
      "         9           0.0074          -0.0000            0.10s\n",
      "        10           0.0076          -0.0000            0.10s\n",
      "        20           0.0068          -0.0001            0.07s\n",
      "        30           0.0060          -0.0000            0.06s\n",
      "        40           0.0068           0.0000            0.05s\n",
      "        50           0.0065           0.0000            0.04s\n",
      "        60           0.0057          -0.0000            0.03s\n",
      "        70           0.0052          -0.0000            0.02s\n",
      "        80           0.0054          -0.0000            0.02s\n",
      "        90           0.0049          -0.0000            0.01s\n",
      "       100           0.0046           0.0000            0.00s\n",
      "[CV]  learning_rate=0.05, n_estimators=100, subsample=0.75, score=-0.055849 -   0.1s\n",
      "[CV] learning_rate=0.05, n_estimators=100, subsample=0.75 ............\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0079           0.0000            0.08s\n",
      "         2           0.0074           0.0000            0.09s\n",
      "         3           0.0069          -0.0000            0.08s\n",
      "         4           0.0072           0.0000            0.09s\n",
      "         5           0.0074          -0.0000            0.09s\n",
      "         6           0.0070          -0.0000            0.08s\n",
      "         7           0.0064           0.0000            0.08s\n",
      "         8           0.0066           0.0000            0.08s\n",
      "         9           0.0068          -0.0000            0.08s\n",
      "        10           0.0070          -0.0000            0.08s\n",
      "        20           0.0060          -0.0000            0.06s\n",
      "        30           0.0058          -0.0000            0.05s\n",
      "        40           0.0058           0.0000            0.04s\n",
      "        50           0.0056          -0.0000            0.03s\n",
      "        60           0.0050          -0.0000            0.03s\n",
      "        70           0.0046          -0.0000            0.02s\n",
      "        80           0.0051          -0.0000            0.01s\n",
      "        90           0.0047          -0.0000            0.01s\n",
      "       100           0.0043          -0.0000            0.00s\n",
      "[CV]  learning_rate=0.05, n_estimators=100, subsample=0.75, score=-0.058486 -   0.1s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0085          -0.0000            0.09s\n",
      "         2           0.0075           0.0000            0.11s\n",
      "         3           0.0077          -0.0000            0.10s\n",
      "         4           0.0081           0.0001            0.10s\n",
      "         5           0.0074          -0.0000            0.10s\n",
      "         6           0.0070          -0.0000            0.10s\n",
      "         7           0.0068          -0.0000            0.10s\n",
      "         8           0.0075          -0.0000            0.09s\n",
      "         9           0.0071          -0.0000            0.09s\n",
      "        10           0.0075          -0.0000            0.09s\n",
      "        20           0.0069          -0.0000            0.07s\n",
      "        30           0.0065          -0.0000            0.06s\n",
      "        40           0.0064          -0.0000            0.05s\n",
      "        50           0.0060          -0.0000            0.04s\n",
      "        60           0.0061          -0.0000            0.04s\n",
      "        70           0.0056          -0.0000            0.03s\n",
      "        80           0.0055          -0.0000            0.02s\n",
      "        90           0.0054          -0.0000            0.01s\n",
      "       100           0.0050          -0.0000            0.00s\n",
      "Param grid:\n",
      "{'n_estimators': [100], 'learning_rate': [0.05], 'subsample': [0.75]}\n",
      "Best Params:\n",
      "{'learning_rate': 0.05, 'n_estimators': 100, 'subsample': 0.75}\n",
      "Best CV Score:\n",
      "0.0578565064244177\n",
      "Best estimator:\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.05, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=2016, subsample=0.75, verbose=1,\n",
      "             warm_start=False)\n",
      "--- Stacker Trained: 2.42 minutes ---\n",
      "time: 2min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "ensemble = Ensemble(n_folds=5,\n",
    "                    stacker=GradientBoostingRegressor(random_state=2016, verbose=1),\n",
    "                    base_models=models)\n",
    "                    \n",
    "model_ensemble = ensemble.fit_predict(x_train[:1000], y_train[:1000], x_valid)\n",
    "\n",
    "#print('MAE', mean_absolute_error(y_valid, ensemble_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 10 and input n_features is 47 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-f8a0f94bd9cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_estimator_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \"\"\"\n\u001b[1;32m   1851\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# for use in inner loop, not raveling the output in single-class case,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m         \u001b[0;31m# not doing input validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0mpredict_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_init_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;34m\"\"\"Check input and compute prediction of ``init``. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    374\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 10 and input n_features is 47 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.7 ms\n"
     ]
    }
   ],
   "source": [
    "model_ensemble.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = transactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "all_preds = pd.DataFrame()\n",
    "for m in test_dates.keys():\n",
    "    # Building predictions.\n",
    "    print('Processing', m)\n",
    "    x_predict = transactions.copy()\n",
    "    x_predict = complex_features(x_predict)\n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    \n",
    "    print('Cleaning data')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict = x_predict.fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict[m] = xgb_gs2.predict(xg.DMatrix(x_predict[best_columns]))\n",
    "    all_preds[m] = x_predict[m].copy()\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict[list(test_dates.keys())] = all_preds[list(test_dates.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = '201610'\n",
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample.to_csv('submission4.csv',index=False)\n",
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {'201610': pd.Timestamp('2016-09-30'),\n",
    "              '201611': pd.Timestamp('2016-10-31'),\n",
    "              '201612': pd.Timestamp('2016-11-30')}\n",
    "\n",
    "x_predict = transactions.copy()\n",
    "x_predict = complex_features(x_predict)\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    print('Processing', m)  \n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    x_predict = x_predict[best_columns].fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    # 5 iterations cat booster for each prediction.\n",
    "    x_predict[m] = cat_booster(x_train, y_train, x_predict.values)\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "del x_predict\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "#new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

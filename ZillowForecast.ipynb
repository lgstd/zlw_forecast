{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import gc \n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 138 ms\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Kernel Data Preparation & Own Implementations\n",
    "\n",
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 780 Âµs\n"
     ]
    }
   ],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Kaggle Kernel Data Preparation\n",
    "\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "drop_tax = False\n",
    "\n",
    "train2016 = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"], low_memory=False)\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "\n",
    "if drop_tax:\n",
    "    # Avoids external bias\n",
    "    print('Removing tax features from 2017')\n",
    "    train2017.iloc[:, train2017.columns.str.startswith('tax')] = np.nan\n",
    "\n",
    "properties2016 = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions2016 = pd.merge(train2016, properties2016, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions2017 = pd.merge(train2017, properties2017, how='left', on=['parcelid']).sample(frac=1)\n",
    "transactions = pd.concat([transactions2016, transactions2017], axis = 0)\n",
    "\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "transactions['taxdelinquencyflag'].replace('Y', 1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.0\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all, y_all, transactions, transactions2016, transactions2017, properties2017, properties2016, train2016, train2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hashottuborspa', 'propertycountylandusecode', 'propertyzoningdesc',\n",
       "       'fireplaceflag', 'taxdelinquencyflag'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.16 ms\n"
     ]
    }
   ],
   "source": [
    "non_number_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Memory usage reduction...\n",
      "Feature engineering...\n",
      "Preparing arrays and throwing out outliers...\n",
      "time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "# Kaggle Kernel Data Preparation\n",
    "\n",
    "print('Loading data...')\n",
    "# Load raw data\n",
    "properties2016_raw = pd.read_csv('../Data/properties_2016.csv', low_memory = False)\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "train2016 = pd.read_csv('../Data/train_2016_v2.csv')\n",
    "train2017 = pd.read_csv('../Data/train_2017.csv')\n",
    "sample_submission = pd.read_csv('../Data/sample_submission.csv', low_memory = False)\n",
    "\n",
    "# Create a new version of 2016 properties data that takes all non-tax variables from 2017\n",
    "taxvars = ['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']\n",
    "tax2016 = properties2016_raw[['parcelid']+taxvars]\n",
    "properties2016 = properties2017.drop(taxvars,axis=1).merge(tax2016, \n",
    "                 how='left', on='parcelid').reindex_axis(properties2017.columns, axis=1)\n",
    "\n",
    "# Create a training data set\n",
    "train2016 = pd.merge(train2016, properties2016, how = 'left', on = 'parcelid')\n",
    "train2017 = pd.merge(train2017, properties2017, how = 'left', on = 'parcelid')\n",
    "train = pd.concat([train2016, train2017], axis = 0)\n",
    "\n",
    "# Create separate test data sets for 2016 and 2017\n",
    "test2016 = pd.merge(sample_submission[['ParcelId']], properties2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "test2017 = pd.merge(sample_submission[['ParcelId']], properties2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "del properties2016, properties2017, train2016, train2017\n",
    "gc.collect();\n",
    "\n",
    "\n",
    "print('Memory usage reduction...')\n",
    "\n",
    "\n",
    "train[['latitude', 'longitude']] /= 1e6\n",
    "train['censustractandblock'] /= 1e12\n",
    "\n",
    "def preptest(test):\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test[['latitude', 'longitude']] /= 1e6\n",
    "    test['censustractandblock'] /= 1e12\n",
    "    test['censustractandblock'] /= 1e12\n",
    "\n",
    "    for column in test.columns:\n",
    "        if test[column].dtype == int:\n",
    "            test[column] = test[column].astype(np.int32)\n",
    "        if test[column].dtype == float:\n",
    "            test[column] = test[column].astype(np.float32)\n",
    "\n",
    "preptest(test2016)\n",
    "preptest(test2017)\n",
    "        \n",
    "print('Feature engineering...')\n",
    "train['month'] = (pd.to_datetime(train['transactiondate']).dt.year - 2016)*12 + pd.to_datetime(train['transactiondate']).dt.month\n",
    "train = train.drop('transactiondate', axis = 1)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "non_number_columns = train.dtypes[train.dtypes == object].index.values\n",
    "\n",
    "for column in non_number_columns:\n",
    "    train_test = pd.concat([train[column], test2016[column], test2017[column]], axis = 0)\n",
    "    encoder = LabelEncoder().fit(train_test.astype(str))\n",
    "    train[column] = encoder.transform(train[column].astype(str)).astype(np.int32)\n",
    "    test2016[column] = encoder.transform(test2016[column].astype(str)).astype(np.int32)\n",
    "    test2017[column] = encoder.transform(test2017[column].astype(str)).astype(np.int32)\n",
    "    \n",
    "feature_names = [feature for feature in train.columns[2:] if feature != 'month']\n",
    "\n",
    "month_avgs = train.groupby('month').agg('mean')['logerror'].values - train['logerror'].mean()\n",
    "\n",
    "X_train_all = train[feature_names].fillna(-1)\n",
    "y_train_all = train['logerror'].fillna(-1)\n",
    "\n",
    "train = train[np.abs(train['logerror']) < 0.4]\n",
    "\n",
    "print('Preparing arrays and throwing out outliers...')\n",
    "X_train = train[feature_names].fillna(-1)#.values\n",
    "y_train = train['logerror'].fillna(-1)#.values\n",
    "X_test2016 = test2016[feature_names].fillna(-1)#.values\n",
    "X_test2017 = test2017[feature_names].fillna(-1)#.values\n",
    "\n",
    "del test2016, test2017;\n",
    "gc.collect();\n",
    "\n",
    "month_values = train['month'].values\n",
    "month_avg_values = np.array([month_avgs[month - 1] for month in month_values]).reshape(-1, 1)\n",
    "X_train = np.hstack([X_train, month_avg_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression MAE: -0.06958 (+/- 0.00384)\n",
      "time: 1.51 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "#model_lr.fit(x_train_data, x_train_label)\n",
    "#y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "#y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "#predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "#predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "\n",
    "#print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_lr, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_lr.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_lr_valid\n",
    "#del y_pred_lr_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BayesianRidge MAE: -0.06952 (+/- 0.00396)\n",
      "time: 2.17 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "#model_br.fit(x_train, y_train)\n",
    "#y_pred_br_valid = model_br.predict(x_valid)\n",
    "#y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['BayesianRidge'] = model_br\n",
    "\n",
    "#predicted_mae_br_valid = mean_absolute_error(y_valid,       y_pred_br_valid)\n",
    "#predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "#print('BR MAE BayesianRidge Valid: %s \\nTrain: %s' % (predicted_mae_br_valid, predicted_mae_br_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_br, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_br.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "\n",
    "#del y_pred_br_valid\n",
    "#del y_pred_br_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  9.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  9.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  9.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5e022d191368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_absolute_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s MAE: %0.5f (+/- %0.5f)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1569\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1571\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1572\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1663\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28min 41s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=1, random_state=2016, verbose=1, n_estimators=500, max_features=12)\n",
    "#model_rf.fit(x_train, y_train)\n",
    "#y_pred_rf_valid = model_rf.predict(x_valid)\n",
    "#y_pred_rf_train = model_rf.predict(x_train_data)\n",
    "models['RandomForest'] = model_rf\n",
    "\n",
    "#predicted_mae_rf_valid = mean_absolute_error(y_valid,       y_pred_rf_valid)\n",
    "#predicted_mae_rf_train = mean_absolute_error(x_train_label, y_pred_rf_train)\n",
    "\n",
    "#print('BR MAE RandomForest Valid: %s \\nTrain: %s' % (predicted_mae_rf_valid, predicted_mae_rf_train))\n",
    "\n",
    "scores = cross_validation.cross_val_score(model_rf, X_train_all, y_train_all, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_rf.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_rf_train\n",
    "#del y_pred_rf_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.18 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "model_et = ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12)\n",
    "\n",
    "#model_et.fit(x_train, y_train)\n",
    "#y_pred_et_valid = model_et.predict(x_valid)\n",
    "#y_pred_et_train = model_et.predict(x_train_data)\n",
    "models['ExtraTrees'] = model_et\n",
    "\n",
    "#predicted_mae_et_valid = mean_absolute_error(y_valid,       y_pred_et_valid)\n",
    "#predicted_mae_et_train = mean_absolute_error(x_train_label, y_pred_et_train)\n",
    "\n",
    "#print('BR MAE ExtraTrees Valid: %s \\nTrain: %s' % (predicted_mae_et_valid, predicted_mae_et_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_et, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_et.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_et_valid\n",
    "#del y_pred_et_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.9 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "model_ab = AdaBoostRegressor()\n",
    "#model_ab.fit(x_train, y_train)\n",
    "#y_pred_ab_valid = model_ab.predict(x_valid)\n",
    "#y_pred_ab_train = model_ab.predict(x_train_data)\n",
    "models['AdaBoost'] = model_ab\n",
    "\n",
    "#predicted_mae_ab_valid = mean_absolute_error(y_valid,       y_pred_ab_valid)\n",
    "#predicted_mae_ab_train = mean_absolute_error(x_train_label, y_pred_ab_train)\n",
    "\n",
    "#print('BR MAE AdaBoost Valid: %s \\nTrain: %s' % (predicted_mae_ab_valid, predicted_mae_ab_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_ab, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_ab.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_ab_valid\n",
    "#del y_pred_ab_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11.2 ms\n"
     ]
    }
   ],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index, loss='MAE'):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=630, learning_rate=0.03,\n",
    "                depth=6, l2_leaf_reg=3,\n",
    "                loss_function=loss,\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_cb, preds = cat_booster(x_train, y_train, x_train_data, x_train_label, cat_index)\n",
    "\n",
    "print('BR MAE CatBoost Valid: %s' % (mean_absolute_error(y_valid, preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12 ms\n"
     ]
    }
   ],
   "source": [
    "model_cb = CatBoostRegressor(\n",
    "            iterations=630, learning_rate=0.03,\n",
    "            depth=6, l2_leaf_reg=3,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE')\n",
    "\n",
    "models['CatBoost'] = model_cb\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_cb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.94 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gb = GradientBoostingRegressor(\n",
    "             random_state=2016, verbose=1,\n",
    "             n_estimators=500, max_features=12, max_depth=8,\n",
    "             learning_rate=0.05, subsample=0.8)\n",
    "\n",
    "#model_gb.fit(x_train, y_train)\n",
    "#y_pred_gb_valid = model_gb.predict(x_valid)\n",
    "#y_pred_gb_train = model_gb.predict(x_train_data)\n",
    "models['GradientBoosting'] = model_gb\n",
    "\n",
    "#predicted_mae_gb_valid = mean_absolute_error(y_valid,       y_pred_gb_valid)\n",
    "#predicted_mae_gb_train = mean_absolute_error(x_train_label, y_pred_gb_train)\n",
    "\n",
    "#print('BR MAE GradientBoosting Valid: %s \\nTrain: %s' % (predicted_mae_gb_valid, predicted_mae_gb_train))\n",
    "\n",
    "#scores = cross_validation.cross_val_score(model_gb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "#print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_gb.__class__.__name__, scores.mean(), scores.std() * 2))\n",
    "\n",
    "#del y_pred_gb_valid\n",
    "#del y_pred_gb_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "#d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "#models['XGB'] = xgb_gs\n",
    "\n",
    "#del d_train\n",
    "#del d_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.15 ms\n"
     ]
    }
   ],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.8 s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,   # num_leaf\n",
    "    'min_data'         : 500,   # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,  # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "d_train = lgb.Dataset(X_train, label=y_train)\n",
    "model_lgb = lgb.train(params_lg, d_train, 430)\n",
    "\n",
    "#model_lgb = light_gbm_folds(x_train, x_train_data, y_train, x_train_label, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neural network data...\n",
      "time: 366 ms\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(X_train)\n",
    "x_train_nn = imputer.transform(X_train)\n",
    "\n",
    "#imputer.fit(x_valid.iloc[:, :])\n",
    "#x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "#x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "models['DNN'] = pipeline\n",
    "\n",
    "#print(mean_absolute_error(y_valid, pipeline.predict(x_valid_nn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "164299/164299 [==============================] - 33s - loss: 0.0527    \n",
      "Epoch 2/15\n",
      "164299/164299 [==============================] - 33s - loss: 0.0525    \n",
      "Epoch 3/15\n",
      "164299/164299 [==============================] - 34s - loss: 0.0526    \n",
      "Epoch 4/15\n",
      "164299/164299 [==============================] - 33s - loss: 0.0525    \n",
      "Epoch 5/15\n",
      "164299/164299 [==============================] - 35s - loss: 0.0525    \n",
      "Epoch 6/15\n",
      "164299/164299 [==============================] - 41s - loss: 0.0525    \n",
      "Epoch 7/15\n",
      "164299/164299 [==============================] - 39s - loss: 0.0525    \n",
      "Epoch 8/15\n",
      "164299/164299 [==============================] - 37s - loss: 0.0525    \n",
      "Epoch 9/15\n",
      "164299/164299 [==============================] - 32s - loss: 0.0525    \n",
      "Epoch 10/15\n",
      "164299/164299 [==============================] - 36s - loss: 0.0525    \n",
      "Epoch 11/15\n",
      "164299/164299 [==============================] - 37s - loss: 0.0525    \n",
      "Epoch 12/15\n",
      "164299/164299 [==============================] - 37s - loss: 0.0525    \n",
      "Epoch 13/15\n",
      "164299/164299 [==============================] - 42s - loss: 0.0525    \n",
      "Epoch 14/15\n",
      "164299/164299 [==============================] - 42s - loss: 0.0525    \n",
      "Epoch 15/15\n",
      "164299/164299 [==============================] - 37s - loss: 0.0525    \n",
      "time: 9min 20s\n"
     ]
    }
   ],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "#x_valid_lstm = x_valid.values.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 100 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.2))\n",
    "lstm.add(Dense(units = 50 , kernel_initializer = 'normal'))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "#validation_data=(x_valid_lstm, y_valid)\n",
    "lstm.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "#yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "#mae = mean_absolute_error(y_valid, yhat)\n",
    "#print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 574 ms\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/dnc1994/Kaggle-Playground/blob/master/home-depot/ensemble.py\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search\n",
    "\n",
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, base_models, floor_models, final_model, include_features, cvgrid, stacker=None):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.final_model = final_model\n",
    "        self.base_models = base_models\n",
    "        self.floor_models = floor_models\n",
    "        self.features = include_features\n",
    "        self.param_grid = cvgrid\n",
    "    \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('Fitting For Base Model {} ---'.format(c))       \n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold %d / %d ---', j + 1, self.n_folds)\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    \n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "\n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "\n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    \n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        if self.features:\n",
    "            S_train = np.append(X, S_train, 1)\n",
    "        \n",
    "        if self.stacker is None:\n",
    "            self.base_models = self.floor_models\n",
    "            self.stacker = self.final_model\n",
    "            self.fit(S_train, y)\n",
    "\n",
    "        d_train = xg.DMatrix(S_train, label=y, missing=-1)\n",
    "        #d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "        xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)\n",
    "\n",
    "        #else:           \n",
    "        #    grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=self.param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        #    grid.fit(S_train, y)\n",
    "        \n",
    "        #try:\n",
    "        #    print('Best Params:')\n",
    "        #    print(grid.best_params_)\n",
    "        #    print('Best CV Score:')\n",
    "        #    print(-grid.best_score_)\n",
    "        #    print('Best estimator:')\n",
    "        #    print(grid.best_estimator_)\n",
    "        #except:\n",
    "        #    pass\n",
    "        \n",
    "        self.stacker = xgb_gs#grid.best_estimator_\n",
    "        \n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        if self.features:\n",
    "            S_test = np.append(X, np.zeros((X.shape[0], len(self.base_models))), 1)  \n",
    "            print('Using features of shape', S_test.shape)\n",
    "        else:\n",
    "            S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "            print('Using features of shape', S_test.shape)\n",
    "\n",
    "        for ind, c in enumerate(self.base_models):\n",
    "            clf = self.base_models[c]\n",
    "            \n",
    "            # Uses all features.\n",
    "            if self.features:\n",
    "                i = X.shape[1] + ind\n",
    "            else:\n",
    "                i = ind\n",
    "                \n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            print('--- Predicting For  #{}'.format(c))\n",
    "            \n",
    "            # Makes predictions for each model\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):    \n",
    "                if c not in ['XGB', 'LSTM']:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(X.reshape((X.shape[0], 1, X.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    S_test_i[:, j] = clf.predict(X)[:]\n",
    "                \n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        try:\n",
    "            y_pred = clf.predict(S_test)[:]\n",
    "        except:\n",
    "            y_pred = clf.predict(xg.DMatrix(S_test))\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test  = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, c in enumerate(self.base_models):\n",
    "            print('########## \\nFitting For Base Model {} \\n##########'.format(c))\n",
    "            clf = self.base_models[c]\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                if c not in ['XGB', 'LightGBM', 'LSTM']:\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_holdout)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    S_test_i[:, j] = clf.predict(T)[:]\n",
    "                    \n",
    "                elif c in ['LSTM']:\n",
    "                    x_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "                    \n",
    "                    clf.fit(x_train_lstm, y_train, epochs=15, batch_size=50, verbose=1, shuffle=False)\n",
    "                    y_pred = clf.predict(X_holdout.reshape((X_holdout.shape[0], 1, X_holdout.shape[1])))[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = [i[0] for i in y_pred]\n",
    "                    S_test_i[:, j] = [i for i in clf.predict(T.reshape((T.shape[0], 1, T.shape[1])))[:]]\n",
    "                    \n",
    "                else:\n",
    "                    d_train = xg.DMatrix(X_train, label=y_train, missing=-1)\n",
    "                    d_valid = xg.DMatrix(X_holdout, missing=-1)\n",
    "                    \n",
    "                    clf = xg.train(params_xgb, d_train)\n",
    "                    y_pred = clf.predict(d_valid)[:]\n",
    "                    \n",
    "                    S_train[test_idx, i] = y_pred\n",
    "                    data_pred = xg.DMatrix(T, missing=-1)\n",
    "                    S_test_i[:, j] = clf.predict(data_pred)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {'n_estimators':  [100],\n",
    "                      'learning_rate': [0.05],\n",
    "                      'subsample':     [0.75]}\n",
    "\n",
    "        #grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        \n",
    "        \n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 765 Âµs\n"
     ]
    }
   ],
   "source": [
    "#del models['CatBoost']\n",
    "#del models['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting For Base Model LinearRegression ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 0.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 0.02 minutes ---\n",
      "Elapsed: 0.02 minutes ---\n",
      "Fitting For Base Model BayesianRidge ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 0.03 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 0.04 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 0.05 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 0.06 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 0.06 minutes ---\n",
      "Elapsed: 0.06 minutes ---\n",
      "Fitting For Base Model RandomForest ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  7.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   10.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 8.07 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  7.9min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   11.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 16.16 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  7.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 24.11 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  7.9min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   13.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 32.23 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  7.9min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    9.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 40.3 minutes ---\n",
      "Elapsed: 40.3 minutes ---\n",
      "Fitting For Base Model ExtraTrees ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 44.34 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    9.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 48.16 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   10.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 52.03 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   13.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 55.98 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:   10.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 59.91 minutes ---\n",
      "Elapsed: 59.91 minutes ---\n",
      "Fitting For Base Model AdaBoost ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 60.12 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 60.39 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 60.65 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 60.89 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 61.09 minutes ---\n",
      "Elapsed: 61.09 minutes ---\n",
      "Fitting For Base Model CatBoost ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 62.18 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 63.31 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 64.55 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 65.75 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 66.94 minutes ---\n",
      "Elapsed: 66.94 minutes ---\n",
      "Fitting For Base Model GradientBoosting ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0067           0.0000            5.51m\n",
      "         2           0.0067           0.0000            5.41m\n",
      "         3           0.0067           0.0000            5.25m\n",
      "         4           0.0066           0.0000            5.16m\n",
      "         5           0.0067           0.0000            4.97m\n",
      "         6           0.0066           0.0000            4.88m\n",
      "         7           0.0066           0.0000            4.82m\n",
      "         8           0.0066           0.0000            4.83m\n",
      "         9           0.0066           0.0000            4.79m\n",
      "        10           0.0066           0.0000            4.80m\n",
      "        20           0.0065           0.0000            4.51m\n",
      "        30           0.0064           0.0000            4.33m\n",
      "        40           0.0063          -0.0000            4.15m\n",
      "        50           0.0062           0.0000            4.00m\n",
      "        60           0.0062           0.0000            3.82m\n",
      "        70           0.0061           0.0000            3.67m\n",
      "        80           0.0061          -0.0000            3.56m\n",
      "        90           0.0061          -0.0000            3.48m\n",
      "       100           0.0061          -0.0000            3.36m\n",
      "       200           0.0057           0.0000            2.44m\n",
      "       300           0.0054           0.0000            1.63m\n",
      "       400           0.0051          -0.0000           49.42s\n",
      "       500           0.0048          -0.0000            0.00s\n",
      "Elapsed: 71.12 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0067           0.0000            5.63m\n",
      "         2           0.0067           0.0000            5.43m\n",
      "         3           0.0066           0.0000            5.32m\n",
      "         4           0.0066           0.0000            5.24m\n",
      "         5           0.0066           0.0000            5.18m\n",
      "         6           0.0066           0.0000            5.19m\n",
      "         7           0.0066           0.0000            5.09m\n",
      "         8           0.0066           0.0000            5.15m\n",
      "         9           0.0066           0.0000            5.15m\n",
      "        10           0.0066           0.0000            5.16m\n",
      "        20           0.0064           0.0000            4.73m\n",
      "        30           0.0064           0.0000            4.49m\n",
      "        40           0.0063           0.0000            4.35m\n",
      "        50           0.0062           0.0000            4.20m\n",
      "        60           0.0062           0.0000            4.07m\n",
      "        70           0.0062          -0.0000            3.94m\n",
      "        80           0.0061          -0.0000            3.81m\n",
      "        90           0.0060          -0.0000            3.71m\n",
      "       100           0.0060          -0.0000            3.58m\n",
      "       200           0.0057          -0.0000            2.54m\n",
      "       300           0.0053          -0.0000            1.68m\n",
      "       400           0.0051          -0.0000           50.79s\n",
      "       500           0.0048          -0.0000            0.00s\n",
      "Elapsed: 75.43 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0068           0.0000            4.86m\n",
      "         2           0.0067           0.0000            4.77m\n",
      "         3           0.0067           0.0000            4.84m\n",
      "         4           0.0067           0.0000            4.90m\n",
      "         5           0.0067           0.0000            4.95m\n",
      "         6           0.0067           0.0000            4.97m\n",
      "         7           0.0066           0.0000            4.96m\n",
      "         8           0.0066           0.0000            4.95m\n",
      "         9           0.0066           0.0000            4.95m\n",
      "        10           0.0066           0.0000            4.96m\n",
      "        20           0.0065           0.0000            4.73m\n",
      "        30           0.0064           0.0000            4.54m\n",
      "        40           0.0063           0.0000            4.42m\n",
      "        50           0.0063           0.0000            4.28m\n",
      "        60           0.0062           0.0000            4.11m\n",
      "        70           0.0062           0.0000            3.94m\n",
      "        80           0.0061          -0.0000            3.77m\n",
      "        90           0.0061          -0.0000            3.66m\n",
      "       100           0.0060          -0.0000            3.55m\n",
      "       200           0.0057          -0.0000            2.55m\n",
      "       300           0.0053          -0.0000            1.70m\n",
      "       400           0.0051          -0.0000           51.42s\n",
      "       500           0.0048          -0.0000            0.00s\n",
      "Elapsed: 79.82 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0067           0.0000            5.79m\n",
      "         2           0.0067           0.0000            5.85m\n",
      "         3           0.0067           0.0000            5.92m\n",
      "         4           0.0067           0.0000            5.81m\n",
      "         5           0.0067           0.0000            5.75m\n",
      "         6           0.0067           0.0000            5.72m\n",
      "         7           0.0067           0.0000            5.60m\n",
      "         8           0.0066           0.0000            5.65m\n",
      "         9           0.0066           0.0000            5.63m\n",
      "        10           0.0067           0.0000            5.61m\n",
      "        20           0.0065           0.0000            5.28m\n",
      "        30           0.0064           0.0000            5.29m\n",
      "        40           0.0063           0.0000            5.07m\n",
      "        50           0.0063           0.0000            4.84m\n",
      "        60           0.0062          -0.0000            4.60m\n",
      "        70           0.0062           0.0000            4.39m\n",
      "        80           0.0062          -0.0000            4.19m\n",
      "        90           0.0061           0.0000            4.02m\n",
      "       100           0.0060          -0.0000            3.86m\n",
      "       200           0.0057          -0.0000            2.69m\n",
      "       300           0.0054          -0.0000            1.80m\n",
      "       400           0.0051          -0.0000           55.24s\n",
      "       500           0.0049          -0.0000            0.00s\n",
      "Elapsed: 84.36 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1           0.0068           0.0000            4.94m\n",
      "         2           0.0067           0.0000            4.68m\n",
      "         3           0.0067           0.0000            4.75m\n",
      "         4           0.0067           0.0000            4.70m\n",
      "         5           0.0067           0.0000            4.65m\n",
      "         6           0.0066           0.0000            4.58m\n",
      "         7           0.0066           0.0000            4.75m\n",
      "         8           0.0066           0.0000            5.00m\n",
      "         9           0.0066           0.0000            5.05m\n",
      "        10           0.0066           0.0000            5.11m\n",
      "        20           0.0065           0.0000            5.08m\n",
      "        30           0.0064           0.0000            4.94m\n",
      "        40           0.0063           0.0000            4.76m\n",
      "        50           0.0063          -0.0000            4.54m\n",
      "        60           0.0062          -0.0000            4.30m\n",
      "        70           0.0062          -0.0000            4.10m\n",
      "        80           0.0061           0.0000            3.89m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        90           0.0061          -0.0000            3.71m\n",
      "       100           0.0060          -0.0000            3.56m\n",
      "       200           0.0057          -0.0000            2.65m\n",
      "       300           0.0054          -0.0000            1.79m\n",
      "       400           0.0051          -0.0000           54.69s\n",
      "       500           0.0049          -0.0000            0.00s\n",
      "Elapsed: 88.93 minutes ---\n",
      "Elapsed: 88.93 minutes ---\n",
      "Fitting For Base Model LightGBM ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 89.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 89.1 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 89.18 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 89.25 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 89.33 minutes ---\n",
      "Elapsed: 89.33 minutes ---\n",
      "Fitting For Base Model DNN ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Elapsed: 91.72 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Elapsed: 94.23 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Elapsed: 96.6 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Elapsed: 99.07 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 5 5\n",
      "Elapsed: 100.98 minutes ---\n",
      "Elapsed: 100.98 minutes ---\n",
      "Fitting For Base Model LSTM ---\n",
      "--- Fitting For Fold %d / %d --- 1 5\n",
      "Epoch 1/15\n",
      "131439/131439 [==============================] - 19s - loss: 0.0525    \n",
      "Epoch 2/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0525    \n",
      "Epoch 3/15\n",
      "131439/131439 [==============================] - 19s - loss: 0.0525    \n",
      "Epoch 4/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Epoch 5/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Epoch 6/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0525    \n",
      "Epoch 7/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0525    \n",
      "Epoch 8/15\n",
      "131439/131439 [==============================] - 24s - loss: 0.0525    \n",
      "Epoch 9/15\n",
      "131439/131439 [==============================] - 24s - loss: 0.0525    \n",
      "Epoch 10/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Epoch 11/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0525    \n",
      "Epoch 12/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0525    \n",
      "Epoch 13/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Epoch 14/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Epoch 15/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0525    \n",
      "Elapsed: 106.37 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 2 5\n",
      "Epoch 1/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0524    \n",
      "Epoch 2/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0524    \n",
      "Epoch 3/15\n",
      "131439/131439 [==============================] - 23s - loss: 0.0524    \n",
      "Epoch 4/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0524    \n",
      "Epoch 5/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0524    \n",
      "Epoch 6/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0524    \n",
      "Epoch 7/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 8/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0524    \n",
      "Epoch 9/15\n",
      "131439/131439 [==============================] - 19s - loss: 0.0524    \n",
      "Epoch 10/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 11/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 12/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 13/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 14/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Epoch 15/15\n",
      "131439/131439 [==============================] - 20s - loss: 0.0524    \n",
      "Elapsed: 111.73 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 3 5\n",
      "Epoch 1/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0526    \n",
      "Epoch 2/15\n",
      "131439/131439 [==============================] - 24s - loss: 0.0526    \n",
      "Epoch 3/15\n",
      "131439/131439 [==============================] - 19s - loss: 0.0526    \n",
      "Epoch 4/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0526    \n",
      "Epoch 5/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0526    \n",
      "Epoch 6/15\n",
      "131439/131439 [==============================] - 21s - loss: 0.0526    \n",
      "Epoch 7/15\n",
      "131439/131439 [==============================] - 22s - loss: 0.0526    \n",
      "Epoch 8/15\n",
      "131439/131439 [==============================] - 27s - loss: 0.0526    \n",
      "Epoch 9/15\n",
      "131439/131439 [==============================] - 27s - loss: 0.0526    \n",
      "Epoch 10/15\n",
      "131439/131439 [==============================] - 28s - loss: 0.0526    \n",
      "Epoch 11/15\n",
      "131439/131439 [==============================] - 27s - loss: 0.0526    \n",
      "Epoch 12/15\n",
      "131439/131439 [==============================] - 28s - loss: 0.0526    \n",
      "Epoch 13/15\n",
      "131439/131439 [==============================] - 27s - loss: 0.0526    \n",
      "Epoch 14/15\n",
      "131439/131439 [==============================] - 25s - loss: 0.0526    \n",
      "Epoch 15/15\n",
      "131439/131439 [==============================] - 27s - loss: 0.0526    \n",
      "Elapsed: 118.01 minutes ---\n",
      "--- Fitting For Fold %d / %d --- 4 5\n",
      "Epoch 1/15\n",
      "131439/131439 [==============================] - 30s - loss: 0.0527    \n",
      "Epoch 2/15\n",
      "131439/131439 [==============================] - 30s - loss: 0.0527    \n",
      "Epoch 3/15\n",
      "131439/131439 [==============================] - 31s - loss: 0.0527    \n",
      "Epoch 4/15\n",
      "131439/131439 [==============================] - 29s - loss: 0.0527    \n",
      "Epoch 5/15\n",
      "131439/131439 [==============================] - 29s - loss: 0.0527    \n",
      "Epoch 6/15\n",
      "131439/131439 [==============================] - 29s - loss: 0.0527    \n",
      "Epoch 7/15\n",
      "131439/131439 [==============================] - 29s - loss: 0.0527    \n",
      "Epoch 8/15\n",
      " 69100/131439 [==============>...............] - ETA: 13s - loss: 0.0528"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators':  [50, 100, 150],\n",
    "              'learning_rate': [0.03, 0.05, 0.07],\n",
    "              'subsample':     [0.5, 0.75, 1]\n",
    "             }\n",
    "\n",
    "stackers = {'GBM' : GradientBoostingRegressor(),}\n",
    "\n",
    "ensemble = Ensemble(n_folds=5,\n",
    "                    base_models=models, \n",
    "                    floor_models=stackers, \n",
    "                    final_model=models['DNN'],\n",
    "                    include_features=False, \n",
    "                    cvgrid=param_grid)\n",
    "                    \n",
    "#model_ensemble = ensemble.fit_predict(x_train[:100], y_train[:100], x_valid)\n",
    "# MAE 0.0653212760898 - lr 0.03, nest = 50, subsample: 0.5\n",
    "ensemble.fit(X_train, y_train)\n",
    "#final_prediction = ensemble.predict(X_test2016)\n",
    "#print('MAE', mean_absolute_error(y_valid, final_prediction))\n",
    "\n",
    "#del final_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(X, K, randomise = False):\n",
    "    \"\"\"Generates K (training, validation) pairs from the items in X.\"\"\"\n",
    "    \n",
    "    if randomise: from random import shuffle; X=list(X); shuffle(X)\n",
    "    for k in range(K):\n",
    "        training   = [x for i, x in enumerate(X) if i % K != k]\n",
    "        validation = [x for i, x in enumerate(X) if i % K == k]\n",
    "        \n",
    "        yield training, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for training, validation in k_fold_cross_validation(x_train_data.values, K=5):\n",
    "    pred_k = ensemble.predict(validation)\n",
    "    print('MAE',mean_absolute_error(x_train_label[index:index+len(validation)], pred_k))\n",
    "    index += len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building properties data')\n",
    "properties2017 = pd.read_csv('../Data/properties_2017.csv', low_memory = False)\n",
    "sample_prediction = pd.merge(sample['ParcelId'].to_frame(), properties2017, how='left', left_on=['ParcelId'], right_on=['parcelid'])\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "sample_prediction['taxdelinquencyflag'].replace('Y', 1, inplace=True)\n",
    "sample_prediction.drop(to_drop, axis=1, inplace=True)\n",
    "sample_prediction = complex_features(sample_prediction)\n",
    "sample_prediction.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1, inplace=True)\n",
    "sample_prediction.fillna(sample_prediction.median(), inplace = True)\n",
    "\n",
    "del properties2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "\n",
    "WEIGHT_XGB = 0.4\n",
    "WEIGHT_CAT = 0.6\n",
    "\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    \n",
    "    print('Processing', m)\n",
    "    sample_prediction['transactiondate'] = test_dates[m]\n",
    "    sample_prediction = time_data(sample_prediction)\n",
    "    \n",
    "    print('Ensemble Prediction', m)\n",
    "    sample_prediction['ensemble'] = ensemble.predict(sample_prediction[best_columns])\n",
    "    \n",
    "    print('XGB - CatBoost Train', m)\n",
    "    predictions_xgb = xgb_gs.predict(xg.DMatrix(sample_prediction[list(best_columns) + ['ensemble']]))\n",
    "    predictions_cat = get_cat_boost_all(sample_train, sample_label, sample_prediction[list(best_columns) + ['ensemble']])\n",
    "\n",
    "    sample[m] = (WEIGHT_XGB * predictions_xgb) + (WEIGHT_CAT * predictions_cat)\n",
    "    \n",
    "    del predictions_xgb, predictions_cat\n",
    "    gc.collect()\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2017 = pd.read_csv('../Data/train_2017.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "sample_train = pd.merge(train2017, sample_prediction, how='left', left_on='parcelid' ,right_on='ParcelId')\n",
    "sample_train['ensemble'] = ensemble.predict(sample_train[best_columns])\n",
    "sample_label = sample_train['logerror']\n",
    "sample_train = time_data(sample_train)[list(best_columns) + ['ensemble']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_xgb = {\n",
    "    'max_depth':        5,  # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1,  # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae'\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(sample_train, label=sample_label)\n",
    "xgb_gs = xg.train(params_xgb, d_train, num_boost_round=250, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_boost_all(x_train, y_train, x_valid):\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "\n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        print('Building ensemble', i)\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "                iterations=600, learning_rate=0.03,\n",
    "                depth=5, l2_leaf_reg=3,\n",
    "                loss_function='MAE',\n",
    "                eval_metric='MAE',\n",
    "                random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        \n",
    "    y_pred_valid /= num_ensembles\n",
    "    \n",
    "    return y_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_prediction.to_csv('submission5.csv',index=False)\n",
    "sample_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "#new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "time: 21.9 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "import xgboost as xg\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, ShuffleSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import cross_validation\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()\n",
    "\n",
    "# Frequency count\n",
    "def get_frequency(data):\n",
    "    # Gets the frequency of a column's values in 'data'. Pass on a series.\n",
    "    vals = pd.merge(data.to_frame(), data.value_counts().reset_index(), \n",
    "                    how='left', left_on=data.to_frame().columns[0], right_on='index').iloc[:, -1:].values\n",
    "    return vals\n",
    "  \n",
    "def time_data(data):\n",
    "    data['transactiondate'] = pd.to_datetime(data['transactiondate'])\n",
    "    data['day_of_week']     = data['transactiondate'].dt.dayofweek\n",
    "    data['month_of_year']   = data['transactiondate'].dt.month\n",
    "    data['quarter']         = data['transactiondate'].dt.quarter\n",
    "    data['is_weekend']      = (data['day_of_week'] < 5).astype(int)\n",
    "    data.drop('transactiondate', axis=1, inplace=True)\n",
    "    \n",
    "    print('Added time data')\n",
    "    print('........')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def column_excluder(data, missing_perc_thresh=0.98):\n",
    "    # Quick clean from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "    \n",
    "    exclude_missing = []\n",
    "    exclude_unique = []\n",
    "    num_rows = data.shape[0]\n",
    "    for c in data.columns:\n",
    "        num_missing = data[c].isnull().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_frac = num_missing / float(num_rows)\n",
    "        if missing_frac > missing_perc_thresh:\n",
    "            exclude_missing.append(c)\n",
    "\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if data[c].isnull().sum() != 0:\n",
    "            num_uniques -= 1\n",
    "        if num_uniques == 1:\n",
    "            exclude_unique.append(c)\n",
    "            \n",
    "    to_exclude = list(set(exclude_missing + exclude_unique))\n",
    "    \n",
    "    print('Excluded columns:')\n",
    "    print(to_exclude)\n",
    "    print('........')\n",
    "    \n",
    "    return to_exclude\n",
    "\n",
    "def categorical_features(data):\n",
    "    # Quick categories from https://www.kaggle.com/seesee/concise-catboost-starter-ensemble-plb-0-06435\n",
    "        \n",
    "    cat_feature_inds = []\n",
    "    cat_unique_thresh = 1000\n",
    "    for i, c in enumerate(data.columns):\n",
    "        num_uniques = len(data[c].unique())\n",
    "        if num_uniques < cat_unique_thresh \\\n",
    "            and not 'sqft'   in c \\\n",
    "            and not 'cnt'    in c \\\n",
    "            and not 'nbr'    in c \\\n",
    "            and not 'number' in c:\n",
    "            cat_feature_inds.append(i)\n",
    "\n",
    "    print(\"Categorical features:\")\n",
    "    print([data.columns[ind] for ind in cat_feature_inds])\n",
    "    print('........')\n",
    "    \n",
    "    return cat_feature_inds\n",
    "\n",
    "\n",
    "def complex_features(data):\n",
    "    # Gets counts, label encoding and frequency estimates.\n",
    "    \n",
    "    # Frequency of occurances | length of codes | check if * is present\n",
    "    data['propertyzoningdesc_frq'] = get_frequency(data['propertyzoningdesc'])\n",
    "    data['propertyzoningdesc_len'] = data['propertyzoningdesc'].apply(lambda x: len(x) if pd.notnull(x) else x)\n",
    "    #transactions_shuffled['propertyzoningdesc_str'] = transactions_shuffled['propertyzoningdesc'].apply(lambda x: (1 if '*' in str(x) else 0) if pd.notnull(x) else x)\n",
    "\n",
    "    # Label encoding | length of code\n",
    "    #transactions_shuffled['propertycountylandusecode_enc'] = transactions_shuffled[['propertycountylandusecode']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    #transactions_shuffled['propertycountylandusecode_len'] = transactions_shuffled['propertycountylandusecode'].apply(lambda x: x if pd.isnull(x) else len(x))\n",
    "\n",
    "    # Zip code area extraction\n",
    "    data['regionidzip_ab']  = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:2]).astype(float)\n",
    "    data['regionidzip_abc'] = data['regionidzip'].apply(lambda x: x if pd.isnull(x) else str(x)[:3]).astype(float)\n",
    "\n",
    "    # Region neighbourhood area extraction\n",
    "    data['regionidneighborhood_ab'] = data['regionidneighborhood'].apply(lambda x: str(x)[:2] if pd.notnull(x) else x).astype(float)\n",
    "\n",
    "    # Rawcensustractandblock transformed\n",
    "    data['code_fips_cnt']  = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[:4]))\n",
    "    data['code_tract_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[4:11]))\n",
    "    data['code_block_cnt'] = get_frequency(data['rawcensustractandblock'].apply(lambda x: str(x)[11:]))\n",
    "    data.drop('rawcensustractandblock', axis=1, inplace=True)\n",
    "    \n",
    "    # Encode string values\n",
    "    data[['propertycountylandusecode', 'propertyzoningdesc']] = data[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Generating complex features')\n",
    "    print('........')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.72 ms\n"
     ]
    }
   ],
   "source": [
    "def cat_booster(x_train, y_train, x_valid, y_valid, cat_index):\n",
    "    # Cat booster train and predict\n",
    "    num_ensembles = 5\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    print('Initialising CAT Boost Regression')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        catb = CatBoostRegressor(\n",
    "            iterations=200, learning_rate=0.01,\n",
    "            depth=3, l2_leaf_reg=3,\n",
    "            loss_function='MAE',\n",
    "            eval_metric='MAE',\n",
    "            random_seed=i)\n",
    "\n",
    "        catb.fit(x_train, y_train, cat_features=cat_index)\n",
    "\n",
    "        y_pred_valid += catb.predict(x_valid)\n",
    "        y_pred_train += catb.predict(x_train)\n",
    "\n",
    "    y_pred_valid /= num_ensembles\n",
    "    y_pred_train /= num_ensembles\n",
    "\n",
    "    print('Train MAE:', mean_absolute_error(y_train, y_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, y_pred_valid))\n",
    "    \n",
    "    return catb, y_pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded columns:\n",
      "['architecturalstyletypeid', 'basementsqft', 'taxdelinquencyyear', 'typeconstructiontypeid', 'pooltypeid7', 'finishedsquarefeet13', 'finishedsquarefeet6', 'pooltypeid10', 'poolcnt', 'storytypeid', 'yardbuildingsqft26', 'hashottuborspa', 'decktypeid', 'poolsizesum', 'taxdelinquencyflag', 'pooltypeid2', 'buildingclasstypeid', 'fireplaceflag']\n",
      "........\n",
      "Added time data\n",
      "........\n",
      "Generating complex features\n",
      "........\n",
      "Categorical features:\n",
      "['airconditioningtypeid', 'buildingqualitytypeid', 'fips', 'heatingorsystemtypeid', 'propertylandusetypeid', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'regionidzip', 'yearbuilt', 'assessmentyear', 'day_of_week', 'month_of_year', 'quarter', 'is_weekend', 'propertyzoningdesc_frq', 'propertyzoningdesc_len', 'regionidzip_ab', 'regionidzip_abc', 'regionidneighborhood_ab']\n",
      "........\n",
      "time: 27.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../Data/properties_2016.csv')\n",
    "sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "\n",
    "transactions = pd.merge(train, prop, how='left', on=['parcelid']).sample(frac=1)\n",
    "#transactions[['propertycountylandusecode', 'propertyzoningdesc']] = transactions[['propertycountylandusecode', 'propertyzoningdesc']].astype(str).apply(LabelEncoder().fit_transform)\n",
    "#transactions['taxdelinquencyflag'].replace('Y',1, inplace=True)\n",
    "    \n",
    "# Clean columns\n",
    "to_drop = column_excluder(transactions)\n",
    "transactions.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Time data\n",
    "transactions = time_data(transactions)\n",
    "transactions = complex_features(transactions)\n",
    "\n",
    "x_all = transactions.drop(['parcelid', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt'], axis=1)\n",
    "y_all = transactions['logerror']\n",
    "#x_all.drop(['hashottuborspa' 'taxdelinquencyflag' 'fireplaceflag'], axis=1)\n",
    "#x_all['hashottuborspa'].astype(float, inplace=True)\n",
    "\n",
    "#x_all.fillna(-1, inplace=True)#.astype(str)#.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "x_all.fillna(x_all.median(),inplace = True)\n",
    "\n",
    "ratio = 0.1\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_all, y_all, test_size=ratio, random_state=54)\n",
    "\n",
    "x_train_label = x_train['logerror'].copy()\n",
    "x_train_data = x_train.drop(['logerror'], axis=1).copy()\n",
    "\n",
    "# Drop outliers \n",
    "x_train = x_train[(x_train['logerror'] > -0.4) & (x_train['logerror'] < 0.419)]\n",
    "y_train = x_train['logerror']\n",
    "x_train.drop('logerror', axis=1, inplace=True)\n",
    "x_valid.drop('logerror', axis=1, inplace=True)\n",
    "\n",
    "cat_index = categorical_features(x_train)\n",
    "best_columns = x_train.columns\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "del x_all\n",
    "del y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 924 µs\n"
     ]
    }
   ],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS MAE LR Valid: 0.066726802458 Train: 0.0680344346439\n",
      "BR MAE LR Valid: 0.0667426692537 Train: 0.0680417227779\n",
      "time: 357 ms\n"
     ]
    }
   ],
   "source": [
    "#### Base features, includes days, tax flag on.\n",
    "# Removed: 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'\n",
    "\n",
    "# OLS\n",
    "    # OLS MAE LR Valid: 0.0671249660894 Train: 0.0684872590475\n",
    "# Bayesian Ridge Regression\n",
    "    # BR MAE LR Valid: 0.0670812710093 Train: 0.0684733515189\n",
    "\n",
    "# Removes null features and ones with little variance\n",
    "\n",
    "# Removes all and processes complex\n",
    "\n",
    "# Best + label encoding\n",
    "\n",
    "# OLS\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(x_train, y_train)\n",
    "y_pred_lr_valid = model_lr.predict(x_valid)\n",
    "y_pred_lr_train = model_lr.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# BayesianRidge Regression\n",
    "model_br = BayesianRidge(compute_score=True)\n",
    "model_br.fit(x_train, y_train)\n",
    "y_pred_br_valid = model_br.predict(x_valid)\n",
    "y_pred_br_train = model_br.predict(x_train_data)\n",
    "models['LinearRegression'] = model_lr\n",
    "\n",
    "# Make predictions on both test and validation with OLS and BR\n",
    "predicted_mae_lr_valid = mean_absolute_error(y_valid, y_pred_lr_valid)\n",
    "predicted_mae_lr_train = mean_absolute_error(x_train_label, y_pred_lr_train)\n",
    "predicted_mae_br_valid = mean_absolute_error(y_valid, y_pred_br_valid)\n",
    "predicted_mae_br_train = mean_absolute_error(x_train_label, y_pred_br_train)\n",
    "\n",
    "print('OLS MAE LR Valid:', predicted_mae_lr_valid, 'Train:', predicted_mae_lr_train)\n",
    "print('BR MAE LR Valid:',  predicted_mae_br_valid, 'Train:', predicted_mae_br_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising CAT Boost Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:31<00:00, 30.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.0521187878452\n",
      "Valid MAE: 0.0662922762167\n",
      "time: 2min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Base features, includes days, tax flag on.\n",
    "# Removed: 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'\n",
    "    #Train MAE: 0.0669406345152\n",
    "    #Valid MAE: 0.0674199299204\n",
    "\n",
    "# Removes null features and ones with little variance\n",
    "\n",
    "# Removes all and processes complex\n",
    "\n",
    "# Best + label encoding\n",
    "model_cb, preds = cat_booster(x_train, y_train, x_valid, y_valid, cat_index)\n",
    "models['CatBoost'] = model_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All scorer objects follow the convention that higher return values are better than lower return \n",
    "# values. Thus metrics which measure the distance between the model and the data, like \n",
    "# metrics.mean_squared_error, are available as neg_mean_squared_error which return the negated \n",
    "# value of the metric.\n",
    "scores = cross_validation.cross_val_score(model_cb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "print(\"%s MAE: %0.5f (+/- %0.5f)\" % (model_cb.__class__.__name__, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:0.053226\tvalid-mae:0.066893\n",
      "Multiple eval metrics have been passed: 'valid-mae' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mae hasn't improved in 100 rounds.\n",
      "[50]\ttrain-mae:0.051906\tvalid-mae:0.066233\n",
      "[100]\ttrain-mae:0.051354\tvalid-mae:0.066211\n",
      "[150]\ttrain-mae:0.050832\tvalid-mae:0.066203\n",
      "Stopping. Best iteration:\n",
      "[95]\ttrain-mae:0.051395\tvalid-mae:0.066191\n",
      "\n",
      "time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "#### Base features, includes days, tax flag on.\n",
    "# Removed: 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'\n",
    "    #train-mae:0.068375\tvalid-mae:0.068989\n",
    "\n",
    "\n",
    "# Removes null features and ones with little variance\n",
    "\n",
    "# Removes all and processes complex\n",
    "\n",
    "# Best + label encoding\n",
    "\n",
    "params_xgb = {\n",
    "    'max_depth':        5, # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        1,  # Ratio of observations to be used as samples for each tree\n",
    "    'min_child_weight': 10, # Deals with imbalanced data and prevents overfitting as the value >\n",
    "    'objective':        'reg:linear',\n",
    "    'n_estimators':     1000, # Sequential trees to be modelled.\n",
    "    'eta':              0.1, # Shrinkage. Typically between 0.1 - 0.2 - learning rate for gradient boost (D:0.3)\n",
    "    'eval_metric':      'mae',\n",
    "    'base_score':       y_mean,\n",
    "}\n",
    "\n",
    "d_train = xg.DMatrix(x_train, label=y_train, missing=-1)\n",
    "d_valid = xg.DMatrix(x_valid, label=y_valid, missing=-1)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "xgb_gs = xg.train(params_xgb, d_train, len(x_valid), watchlist, early_stopping_rounds=100, verbose_eval=50)\n",
    "models['XGB'] = xgb_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.01 ms\n"
     ]
    }
   ],
   "source": [
    "def light_gbm_folds(x_train, x_valid, y_train, y_valid, params, num_ensembles):\n",
    "    # Light gbm n ensambles average predictions\n",
    "\n",
    "    y_pred_valid = 0.0\n",
    "    y_pred_train = 0.0\n",
    "    \n",
    "    d_train = lgb.Dataset(x_train, label=y_train)\n",
    "    \n",
    "    print('Initialising Light GBM')\n",
    "    for i in tqdm(range(num_ensembles)):\n",
    "        # Use CV, tune hyperparameters\n",
    "        params['seed'] = i\n",
    "        model_lgb = lgb.train(params, d_train, 430)\n",
    "        \n",
    "        lg_pred_valid = model_lgb.predict(x_valid)\n",
    "        lg_pred_train = model_lgb.predict(x_train)\n",
    "\n",
    "    lg_pred_valid /= num_ensembles\n",
    "    lg_pred_train /= num_ensembles\n",
    "    \n",
    "    print('Train MAE:', mean_absolute_error(y_train, lg_pred_train))\n",
    "    print('Valid MAE:', mean_absolute_error(y_valid, lg_pred_valid))\n",
    "    \n",
    "    return model_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Light GBM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:00<00:00, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MAE: 0.0532560595725\n",
      "Valid MAE: 0.0669780442807\n",
      "time: 1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "\n",
    "params_lg={\n",
    "    'max_bin'          : 10,\n",
    "    'learning_rate'    : 0.0021, # shrinkage_rate\n",
    "    'boosting_type'    : 'gbdt',\n",
    "    'objective'        : 'regression',\n",
    "    'metric'           : 'mae',      \n",
    "    'sub_feature'      : 0.345 ,   \n",
    "    'bagging_fraction' : 0.85, \n",
    "    'bagging_freq'     : 40,\n",
    "    'num_leaves'       : 512,       # num_leaf\n",
    "    'min_data'         : 500,         # min_data_in_leaf\n",
    "    'min_hessian'      : 0.05,     # min_sum_hessian_in_leaf\n",
    "    'verbose'          : 1\n",
    "}\n",
    "\n",
    "model_lgb = light_gbm_folds(x_train, x_valid, y_train, y_valid, params_lg, num_ensembles=5)\n",
    "models['LightGBM'] = model_lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 52.5 ms\n"
     ]
    }
   ],
   "source": [
    "#### Base features, includes days, tax flag on.\n",
    "# Removed: 'propertyzoningdesc','propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'\n",
    "# NN\n",
    "\n",
    "# Removes null features and ones with little variance\n",
    "\n",
    "# Removes all and processes complex\n",
    "\n",
    "# Best + label encoding\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.noise import GaussianDropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size*2, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "# define wider model\n",
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size*2, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(size, input_dim=size, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "    return model\n",
    "\n",
    "def prebuilt_nn():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(units = 400 , kernel_initializer = 'normal', input_dim = size))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(Dropout(.4))\n",
    "    nn.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(units = 64 , kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.5))\n",
    "    nn.add(Dense(units = 26, kernel_initializer = 'normal'))\n",
    "    nn.add(PReLU())\n",
    "    nn.add(BatchNormalization())\n",
    "    nn.add(Dropout(.6))\n",
    "    nn.add(Dense(1, kernel_initializer='normal'))\n",
    "    nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing neural network data...\n",
      "time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "print(\"Preprocessing neural network data...\")\n",
    "imputer= Imputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train_nn = imputer.transform(x_train.iloc[:, :])\n",
    "\n",
    "imputer.fit(x_valid.iloc[:, :])\n",
    "x_valid_nn = imputer.transform(x_valid.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train_nn = sc.fit_transform(x_train_nn)\n",
    "x_valid_nn = sc.transform(x_valid_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.066781628587123346"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "size = x_train_nn.shape[1]\n",
    "# Prebuit KAGGLE Kernel\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=prebuilt_nn, epochs=5, batch_size=50, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "pipeline.fit(x_train_nn, y_train)\n",
    "mean_absolute_error(y_valid, pipeline.predict(x_valid_nn))\n",
    "models['DNN'] = pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79677 samples, validate on 9028 samples\n",
      "Epoch 1/10\n",
      "79677/79677 [==============================] - 14s - loss: 0.0556 - val_loss: 0.0705\n",
      "Epoch 2/10\n",
      "79677/79677 [==============================] - 11s - loss: 0.0549 - val_loss: 0.0670\n",
      "Epoch 3/10\n",
      "79677/79677 [==============================] - 12s - loss: 0.0545 - val_loss: 0.0669\n",
      "Epoch 4/10\n",
      "79677/79677 [==============================] - 12s - loss: 0.0541 - val_loss: 0.0670\n",
      "Epoch 5/10\n",
      "79677/79677 [==============================] - 13s - loss: 0.0538 - val_loss: 0.0670\n",
      "Epoch 6/10\n",
      "79677/79677 [==============================] - 12s - loss: 0.0537 - val_loss: 0.0671\n",
      "Epoch 7/10\n",
      "79677/79677 [==============================] - 12s - loss: 0.0535 - val_loss: 0.0670\n",
      "Epoch 8/10\n",
      "79677/79677 [==============================] - 12s - loss: 0.0534 - val_loss: 0.0669\n",
      "Epoch 9/10\n",
      "79677/79677 [==============================] - 13s - loss: 0.0534 - val_loss: 0.0670\n",
      "Epoch 10/10\n",
      "79677/79677 [==============================] - 14s - loss: 0.0534 - val_loss: 0.0670\n",
      "Test MAE: 0.067\n",
      "time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "#x_train = x_train.values\n",
    "#x_valid = x_valid.values\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "x_train_lstm = x_train.reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_valid_lstm = x_valid.reshape((x_valid.shape[0], 1, x_valid.shape[1]))\n",
    " \n",
    "# design network\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(50, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
    "lstm.add(PReLU())\n",
    "lstm.add(Dropout(.4))\n",
    "lstm.add(Dense(units = 160 , kernel_initializer = 'normal'))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "lstm.fit(x_train_lstm, y_train, epochs=10, batch_size=50, validation_data=(x_valid_lstm, y_valid), verbose=1, shuffle=False)\n",
    " \n",
    "# make a prediction\n",
    "yhat = lstm.predict(x_valid_lstm)\n",
    "models['LSTM'] = lstm\n",
    "mae = mean_absolute_error(y_valid, yhat)\n",
    "print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 31.2 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, \\\n",
    "        ExtraTreesRegressor, AdaBoostClassifier\n",
    "from sklearn import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 192 ms\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error_(ground_truth, predictions):\n",
    "    return mean_absolute_error(ground_truth, predictions)\n",
    "\n",
    "MAE = make_scorer(mean_absolute_error_, greater_is_better=False)\n",
    "\n",
    "class Ensemble(object):\n",
    "    \n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        start_time = time.time()\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            print('Fitting For Base Model #%d / %d ---', i+1, len(self.base_models))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "                print('--- Fitting For Fold %d / %d ---', j+1, self.n_folds)\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]\n",
    "                S_train[test_idx, i] = y_pred\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        clf = self.stacker\n",
    "        clf.fit(S_train, y)\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "    def preidct(self, X):\n",
    "        X = np.array(X)\n",
    "        folds = list(KFold(len(X), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "        S_test = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            S_test_i = np.zeros((X.shape[0], len(folds)))\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                S_test_i[:, j] = clf.predict(X)[:]\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        clf = self.stacker\n",
    "        y_pred = clf.predict(S_test)[:]\n",
    "        return y_pred\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        start_time = time.time()\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            print('Fitting For Base Model #{0} / {1} ---'.format(i+1, len(self.base_models)))\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "\n",
    "                print('--- Fitting For Fold #{0} / {1} ---'.format(j+1, self.n_folds))\n",
    "\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                \n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]\n",
    "                \n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict(T)[:]\n",
    "\n",
    "                print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "            print('Elapsed: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        print('--- Base Models Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [100],\n",
    "            'learning_rate': [0.05],\n",
    "            'subsample': [0.75]\n",
    "        }\n",
    "        grid = grid_search.GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, verbose=20, scoring=MAE)\n",
    "        grid.fit(S_train, y)\n",
    "\n",
    "        # a little memo\n",
    "        message = 'to determine local CV score of #28'\n",
    "\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('--- Stacker Trained: %s minutes ---' % round(((time.time() - start_time) / 60), 2))\n",
    "\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "base_models = [\n",
    "    RandomForestRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12\n",
    "    ),\n",
    "    ExtraTreesRegressor(\n",
    "        n_jobs=1, random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12\n",
    "    ),\n",
    "    GradientBoostingRegressor(\n",
    "        random_state=2016, verbose=1,\n",
    "        n_estimators=500, max_features=12, max_depth=8,\n",
    "        learning_rate=0.05, subsample=0.8\n",
    "    ),\n",
    "    XGBRegressor(\n",
    "        seed=2016,\n",
    "        n_estimators=200, max_depth=8,\n",
    "        learning_rate=0.05, subsample=0.8, colsample_bytree=0.85\n",
    "    )\n",
    "]\n",
    "\n",
    "ensemble = Ensemble(\n",
    "    n_folds=5,\n",
    "    stacker=GradientBoostingRegressor(\n",
    "        random_state=2016, verbose=1\n",
    "    ),\n",
    "    base_models=base_models#models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting For Base Model #1 / 4 ---\n",
      "--- Fitting For Fold #1 / 5 ---\n"
     ]
    }
   ],
   "source": [
    "ensemble_prediction = ensemble.fit_predict(x_train, y_train, x_valid)\n",
    "print('MAE', mean_absolute_error(y_valid, ensemble_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########## LAYER 1 ##########\n",
    "# Submodel  1 : OLS                      # Ordinary least squares estimator Sklearn implementation\n",
    "# Submodel  2 : BR                       # Bayesian ridge regression - Sklearn implementation\n",
    "# Submodel  3 : DNN                      # Dense Neural Network - Keras - Dense layers \n",
    "# Submodel  4 : LightGBM                 # Light Gradient Boosting - https://github.com/Microsoft/LightGBM\n",
    "# Submodel  5 : XGBoost                  # Extreme Gradient Boosting - http://xgboost.readthedocs.io/en/latest/model.html\n",
    "# Submodel  6 : CatBoost                 # Categorical Boosting https://github.com/catboost/catboost\n",
    "# Submodel  7 : LSTM                     # Long Short Term Memory Neural Network - Keras implementation\n",
    "# Submodel  8 : RandomForestRegressor    # Sklearn implementation\n",
    "# Submodel  9 : ExtraTreesRegressor      # Sklearn implementation\n",
    "# Submodel 10 : SVR                      # Support vector machines for regression - Sklearn implementation\n",
    "# Submodel 11 : AdaBoost                 # Adaptive Boosting Sklearn Implementation\n",
    "\n",
    "########## LAYER 2 ##########\n",
    "# https://www.kaggle.com/dragost/boosted-trees-lb-0-0643707/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = transactions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "#    '201710': pd.Timestamp('2017-09-30'),\n",
    "#   '201711': pd.Timestamp('2017-10-31'),\n",
    "#    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "\n",
    "all_preds = pd.DataFrame()\n",
    "for m in test_dates.keys():\n",
    "    # Building predictions.\n",
    "    print('Processing', m)\n",
    "    x_predict = transactions.copy()\n",
    "    x_predict = complex_features(x_predict)\n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    \n",
    "    print('Cleaning data')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict = x_predict.fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    x_predict[m] = xgb_gs2.predict(xg.DMatrix(x_predict[best_columns]))\n",
    "    all_preds[m] = x_predict[m].copy()\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "#del x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict[list(test_dates.keys())] = all_preds[list(test_dates.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = '201610'\n",
    "submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample.to_csv('submission4.csv',index=False)\n",
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {'201610': pd.Timestamp('2016-09-30'),\n",
    "              '201611': pd.Timestamp('2016-10-31'),\n",
    "              '201612': pd.Timestamp('2016-11-30')}\n",
    "\n",
    "x_predict = transactions.copy()\n",
    "x_predict = complex_features(x_predict)\n",
    "\n",
    "for m in test_dates.keys():\n",
    "    print('Processing', m)  \n",
    "    x_predict['transactiondate'] = test_dates[m]\n",
    "    x_predict = time_data(x_predict)\n",
    "    x_predict = x_predict[best_columns].fillna(-999).astype(str).apply(LabelEncoder().fit_transform)\n",
    "    \n",
    "    print('Predicting')\n",
    "    print('........')\n",
    "    \n",
    "    # 5 iterations cat booster for each prediction.\n",
    "    x_predict[m] = cat_booster(x_train, y_train, x_predict.values)\n",
    "    submission_sample[m] = submission_sample['ParcelId'].to_frame().merge(x_predict[['parcelid', m]], how='left', left_on='ParcelId', right_on='parcelid')[m]\n",
    "    \n",
    "del x_predict\n",
    "del predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#model_lr = LinearRegression()\n",
    "model_xgb = xg.XGBRegressor()\n",
    "selector = RFE(model_xgb, 100, step=500)\n",
    "model = selector.estimator.fit(x_train, y_train)\n",
    "\n",
    "dict_features = plot_best_features(model, data=x_all, num_features=100, figsize=(5,15))\n",
    "\n",
    "best_columns = list(dict_features.keys())\n",
    "#new_sparse_columns = x_all.columns\n",
    "x_train = pd.DataFrame(x_train, columns=x_all.columns)[best_columns].values\n",
    "x_test  = pd.DataFrame(x_test,  columns=x_all.columns)[best_columns].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('<div id=\"toc\"></div>').css({position: 'fixed', top: '120px', left: 0}).appendTo(document.body);\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autotime\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "properties_data = pd.read_csv('../Data/properties_2016.csv', low_memory=False)\n",
    "# Load train data\n",
    "train_data = pd.read_csv('../Data/train_2016_v2.csv')\n",
    "# Elements to be forecasted - this is the framework\n",
    "submission_sample = pd.read_csv('../Data/sample_submission.csv')\n",
    "# Load label description and feature documentation\n",
    "label_documentation = pd.read_csv('../Data/zillow_data_dictionary.csv', encoding='ISO8859_1')\n",
    "# Replace null values, identify duplicates.\n",
    "transactions = pd.merge(train_data, properties_data, how='left', on=['parcelid'])\n",
    "duplicate_records = train_data[train_data['parcelid'].duplicated()]['parcelid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many nulls there are for each feature\n",
    "properties_data.isnull().sum().sort_values(ascending=False).to_frame().plot.barh(figsize=(7,13), title='Nulls per feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transactions['latitude'] = transactions['latitude']/1000000\n",
    "transactions['longitude'] = transactions['longitude']/1000000\n",
    "\n",
    "# Get latitude and longitude extremes\n",
    "min_lat = transactions['latitude'].min()\n",
    "max_lat = transactions['latitude'].max()\n",
    "min_lon = transactions['longitude'].min()\n",
    "max_lon = transactions['longitude'].max()\n",
    "\n",
    "# Build map\n",
    "area = 0.1\n",
    "fig = plt.figure(figsize=(40,40))\n",
    "map = Basemap(projection='merc', lat_0 = np.mean([min_lat, max_lat]), lon_0 = np.mean([min_lon, max_lon]),\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=min_lon - area, llcrnrlat=min_lat - area,\n",
    "    urcrnrlon=max_lon + area, urcrnrlat=max_lat + area)\n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    " \n",
    "lon = transactions['longitude'].values\n",
    "lat = transactions['latitude'].values\n",
    "x,y = map(lon, lat)\n",
    "map.plot(x, y, 'bo', markersize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(200,50))\n",
    "plt.plot(transactions['logerror'], linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(test, pred, sample, title, width=40, height=10, linewidth=0.5, color1='white', color2='orange'):\n",
    "    \"\"\" Plotting method. \"\"\"\n",
    "    fig = plt.figure(figsize=(width, height))\n",
    "    plt.plot(pred[:sample], color=color1, zorder=4, linewidth=linewidth, label='%s Prediction'%(title))\n",
    "    plt.plot(test[:sample], color=color2, zorder=3, linewidth=linewidth, label='%s True Data'%(title))\n",
    "    plt.title = title\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some set of features which would intuitively make sense to be correlated with the price of the house.\n",
    "# However, given that the problem is to discover areas of improvement in the model, they might not contribute\n",
    "# significantly. \n",
    "temporal = ['month_of_year', 'quarter']#,'day_of_week','is_weekend']\n",
    "ids      = ['storytypeid','airconditioningtypeid','buildingclasstypeid','typeconstructiontypeid',\n",
    "            'architecturalstyletypeid','propertylandusetypeid','decktypeid','pooltypeid10',\n",
    "            'pooltypeid2','buildingqualitytypeid','pooltypeid7','heatingorsystemtypeid',\n",
    "            'code_fips','code_tract','code_block']\n",
    "regional = ['regionidcounty','regionidcity','fips']#'regionidzip','regionidneighborhood',\n",
    "cnts     = ['bathroomcnt', 'fullbathcnt', 'bedroomcnt','threequarterbathnbr',\n",
    "            'numberofstories','garagecarcnt','roomcnt','fireplacecnt','calculatedbathnbr',\n",
    "            'unitcnt','poolcnt']\n",
    "surfaces = ['finishedsquarefeet12', 'calculatedfinishedsquarefeet',\n",
    "            'finishedsquarefeet50','basementsqft','lotsizesquarefeet',\n",
    "            'finishedsquarefeet13','yardbuildingsqft17','garagetotalsqft',\n",
    "            'finishedfloor1squarefeet','yardbuildingsqft26','finishedsquarefeet15',\n",
    "            'poolsizesum','finishedsquarefeet6']\n",
    "taxes    = ['structuretaxvaluedollarcnt','taxvaluedollarcnt','taxdelinquencyflag','taxdelinquencyyear',\n",
    "            'landtaxvaluedollarcnt','taxamount']\n",
    "\n",
    "# Columns to be binarised.\n",
    "dummies = ['taxdelinquencyyear'] + regional + ids\n",
    "#descript = ['propertyzoningdesc']\n",
    "other    = ['age','latitude','longitude']#'yearbuilt']\n",
    "\n",
    "transactions_final_columns = temporal + ids + regional + cnts + surfaces + taxes + other\n",
    "\n",
    "# Building time features\n",
    "transactions['transactiondate'] = pd.to_datetime(transactions['transactiondate'])\n",
    "transactions['day_of_week'] = transactions['transactiondate'].dt.dayofweek\n",
    "transactions['month_of_year'] = transactions['transactiondate'].dt.month\n",
    "transactions['quarter'] = transactions['transactiondate'].dt.quarter\n",
    "transactions['is_weekend'] = (transactions['day_of_week'] < 5).astype(int)\n",
    "transactions['year'] = transactions['transactiondate'].dt.year\n",
    "transactions['age'] = transactions['year'] - transactions['yearbuilt']\n",
    "# Other features\n",
    "transactions['taxdelinquencyflag'] = transactions['taxdelinquencyflag'].replace('Y', 1)\n",
    "transactions['code_fips']  = transactions['rawcensustractandblock'].apply(lambda x: str(x)[:4])\n",
    "transactions['code_tract'] = transactions['rawcensustractandblock'].apply(lambda x: str(x)[4:11])\n",
    "transactions['code_block'] = transactions['rawcensustractandblock'].apply(lambda x: str(x)[11:])\n",
    "# Feature importance based on its correlation with the 'logerror'\n",
    "# corrs = transactions.corr()['logerror'].sort_values(ascending=False)\n",
    "transactions = transactions.fillna(0)\n",
    "# Build train features\n",
    "transactions_shuffled = transactions.sample(frac=1)\n",
    "\n",
    "# Get sparse data\n",
    "x_all = pd.get_dummies(transactions_shuffled[transactions_final_columns], columns=dummies)\n",
    "#x_all = transactions_shuffled[transactions_final_columns]\n",
    "y_all = transactions_shuffled['logerror'].values\n",
    "\n",
    "new_sparse_columns = x_all.columns\n",
    "# Splits up train and test based on the given ration\n",
    "ratio = 0.1\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all.values, y_all, test_size=ratio, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show transaction distribution over time.\n",
    "fig = plt.figure(figsize=(20,9))\n",
    "counts = transactions.groupby(['month_of_year','year']).count()['parcelid'].reset_index()\n",
    "plt.bar(counts['month_of_year'].values, counts['parcelid'].values)\n",
    "plt.title('Transaction ditribution over 12 months.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "# Fir the model to train data\n",
    "model_lr.fit(x_train, y_train)\n",
    "# Make a prediction for the test set\n",
    "y_pred_lr_test = model_lr.predict(x_test)\n",
    "y_pred_lr_train = model_lr.predict(x_train)\n",
    "# Score the predictor on the test set\n",
    "model_lr.score(x_test, y_test)\n",
    "# Feature importance\n",
    "feature_importance_lr = pd.DataFrame(model_lr.coef_, columns=['Weight'], index=new_sparse_columns).sort_values('Weight', ascending=False)\n",
    "# Same R2 computation based but based on metrics library\n",
    "print('R2 LR Test:', r2_score(y_test, y_pred_lr_test), 'Train:', r2_score(y_train, y_pred_lr_train))\n",
    "\n",
    "predicted_mae_lr_test = mean_absolute_error(y_test, y_pred_lr_test)\n",
    "predicted_mae_lr_train = mean_absolute_error(y_train, y_pred_lr_train)\n",
    "print('MAE LR  Train:', predicted_mae_lr_train, 'Test:',predicted_mae_lr_test)\n",
    "\n",
    "sample = 100 # Number of records to look at - makes the visualisation more meaningful.\n",
    "# Plot test true vs predicted values\n",
    "plot_data(y_test, y_pred_lr_test, sample, 'Test', linewidth=2)\n",
    "# Plot train true vs predicted values\n",
    "plot_data(y_train, y_pred_lr_train, sample, 'Train', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBModel\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "def plot_best_features(model, data, num_features, figsize=(5,50)):\n",
    "    \"\"\" \n",
    "    Plot best features. \n",
    "    \n",
    "    Args:\n",
    "        model (XGBRegressor) : The best XGBRegressor estimator from GridSearchCV or other \n",
    "                               model of type XGBRegressor.\n",
    "        data     (DataFrame) : Data containing all features and column names.\n",
    "        \n",
    "    Returns:\n",
    "        dict : The newly created dictionary, which maps 'data' features to their associated score.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    new_scores = {}\n",
    "    # Get the XGB Model's score and assign values and keys to the new dictionary\n",
    "    scores = model.booster().get_score(importance_type='weight')\n",
    "    for i in scores.keys():\n",
    "        new_scores[data.columns[int(i[1:])]] = scores[i]\n",
    "    # Build a dataframe with the top 'num_features'\n",
    "    df_features = pd.DataFrame.from_records([new_scores], index=['Features']).T.sort_values('Features').tail(num_features)\n",
    "    # Plot feature significance based on the models' score\n",
    "    df_features.plot.barh(figsize=figsize)\n",
    "    \n",
    "    return new_scores\n",
    "\n",
    "\n",
    "cv = 5\n",
    "jobs = 10\n",
    "params={\n",
    "    'max_depth':        [3], # shuld be 0.5 to 1% of the examples\n",
    "    'subsample':        [0.8, 1], #[0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "    'min_child_weight': [10],\n",
    "    #'colsample_bytree': [0.5], #[0.5,0.6,0.7,0.8],\n",
    "    'objective':        ['reg:linear'],\n",
    "    'n_estimators':     [1000], #[1000,2000,3000]\n",
    "    'reg_alpha':        [0], #[0.01, 0.02, 0.03, 0.04]\n",
    "    'learning_rate':    [0.1]\n",
    "}\n",
    "\n",
    "# Build XGB model based on the given parameters.\n",
    "# Default features:\n",
    "# max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='reg:linear', \n",
    "# booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, \n",
    "# subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
    "# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None\n",
    "xgbr = xg.XGBRegressor()\n",
    "xgb_gs =GridSearchCV(xgbr, params, n_jobs=jobs, \n",
    "                   cv=TimeSeriesSplit(n_splits=cv).get_n_splits([x_train,y_train]), \n",
    "                   #scoring='neg_mean_absolute_error',\n",
    "                   verbose=1, refit=True)\n",
    "\n",
    "xgb_gs.fit(x_train, y_train)\n",
    "print('Best estimator:',xgb_gs.best_estimator_)\n",
    "# Predict estimated logerror\n",
    "y_pred_xgb_test = xgb_gs.predict(x_test)\n",
    "y_pred_xgb_train = xgb_gs.predict(x_train)\n",
    "# Evaluate the performance of XGB\n",
    "print('XGB R2 Test:', xgb_gs.score(x_test, y_test), 'Train:', xgb_gs.score(x_train, y_train))\n",
    "\n",
    "# Show results for LR on train and test data\n",
    "print('MAE LR  Train:', predicted_mae_lr_train, 'Test:',predicted_mae_lr_test)\n",
    "\n",
    "# Show results for XGB on train and test data\n",
    "predicted_mae_xgb_test = mean_absolute_error(y_test, y_pred_xgb_test)\n",
    "predicted_mae_xgb_train = mean_absolute_error(y_train, y_pred_xgb_train)\n",
    "print('MAE XGB Train:', predicted_mae_xgb_train, 'Test:',predicted_mae_xgb_test)\n",
    "\n",
    "#selector = RFE(xgb_gs.best_estimator_, 100, step=50)\n",
    "#selector = selector.fit(x_train, y_train)\n",
    "\n",
    "# plot feature importance\n",
    "dict_features = plot_best_features(xgb_gs.best_estimator_, data=x_all, num_features=50, figsize=(5,13))\n",
    "#xgb.to_graphviz(xgb_gs.best_estimator_, num_trees=50)\n",
    "\n",
    "######## ONE-HOT vs NONE ######\n",
    "# ONE-HOT:\n",
    "# XGB R2 Test: -0.0135715900237 Train: 0.13302709163\n",
    "# MAE LR  Train: 0.0687819933569 Test: 0.0675078152125\n",
    "# MAE XGB Train: 0.0669039183129 Test: 0.0687189110259\n",
    "\n",
    "# NONE:\n",
    "# XGB R2 Test: 0.011877997183 Train: 0.141170034298\n",
    "# MAE LR  Train: 0.0680096936624 Test: 0.0713713207428\n",
    "# MAE XGB Train: 0.0662614095391 Test: 0.0722176694789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
